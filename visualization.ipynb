{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model, parameters, performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from models_fine.vqvae import VQVAE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "\n",
    "def load_model(model_filename):\n",
    "    path = os.getcwd() + '/results/'\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        data = torch.load(path + model_filename,  weights_only=False)\n",
    "    else:\n",
    "        data = torch.load(path+model_filename,map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    params = data[\"hyperparameters\"]\n",
    "    \n",
    "    model = VQVAE(params['n_hiddens'], params['n_residual_hiddens'],\n",
    "                  params['n_residual_layers'], params['n_embeddings'], \n",
    "                  params['embedding_dim'], params['beta']).to(device)\n",
    "\n",
    "    model.load_state_dict(data['model'])\n",
    "    \n",
    "    return model, data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End of utilities\n",
    "\"\"\"\n",
    "\n",
    "model_filename = 'vqvae_data_thu_jun_19_16_39_23_2025.pth'\n",
    "\n",
    "model,vqvae_data = load_model(model_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "training_data, validation_data, training_loader, validation_loader, x_train_var = utils.load_data_and_data_loaders('CIFAR10', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'n_updates': 30000,\n",
       " 'n_hiddens': 128,\n",
       " 'n_residual_hiddens': 32,\n",
       " 'n_residual_layers': 2,\n",
       " 'embedding_dim': 64,\n",
       " 'n_embeddings': 512,\n",
       " 'beta': 0.25,\n",
       " 'learning_rate': 0.0003,\n",
       " 'log_interval': 50,\n",
       " 'dataset': 'CIFAR10',\n",
       " 'save': True,\n",
       " 'filename': 'thu_jun_19_16_39_23_2025'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = vqvae_data['hyperparameters']\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def update_codebook_with_cga(model, z_e_all, ei_all, ej_all, usage_counts, bottom_percent=0.6):\n",
    "    \"\"\"\n",
    "    Perform genetic algorithm-based update for underutilized codebook tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - model: VQVAE model with .vector_quantization.embedding.weight\n",
    "    - z_e_all: (N, D) encoder outputs flattened\n",
    "    - ei_all: (N,) top-1 encoding indices\n",
    "    - ej_all: (N,) top-2 encoding indices\n",
    "    - usage_counts: (K,) usage count of each codebook vector\n",
    "    - bottom_percent: float, percentage of least-used tokens to consider for update\n",
    "    \"\"\"\n",
    "    device = z_e_all.device\n",
    "    codebook = model.vector_quantization.embedding.weight  # (K, D)\n",
    "    K = codebook.shape[0]\n",
    "\n",
    "    # Identify bottom X% underutilized codeword indices\n",
    "    num_bottom = int(K * bottom_percent)\n",
    "    _, sorted_indices = torch.sort(usage_counts)\n",
    "    underutilized_tokens = sorted_indices[:num_bottom].tolist()\n",
    "\n",
    "    # For collecting e_lo vectors for each underutilized codeword\n",
    "    update_vectors = {k: [] for k in underutilized_tokens}\n",
    "\n",
    "    for z_e, ei, ej in zip(z_e_all, ei_all, ej_all):\n",
    "        if ei.item() not in underutilized_tokens:\n",
    "            continue  # skip if token is not underutilized\n",
    "\n",
    "        e_i = codebook[ei]  # (D,)\n",
    "        e_j = codebook[ej]  # (D,)\n",
    "\n",
    "        # Crossover\n",
    "        alpha = torch.rand(1).item()\n",
    "        e_cross = alpha * e_i + (1 - alpha) * e_j\n",
    "\n",
    "        # Mutation\n",
    "        beta = torch.empty(1).uniform_(-2, 2).item()\n",
    "        e_mut = beta * e_cross\n",
    "\n",
    "        # Local Search\n",
    "        epsilon = torch.randn_like(e_mut)\n",
    "        s = 1.0\n",
    "        e_lo = e_mut + s * epsilon\n",
    "        dist_ei = F.mse_loss(z_e, e_i)\n",
    "        dist_lo = F.mse_loss(z_e, e_lo)\n",
    "\n",
    "        # Find smallest s such that distance condition met\n",
    "        tries = 10\n",
    "        while dist_lo >= dist_ei and tries > 0:\n",
    "            s *= 0.5\n",
    "            e_lo = e_mut + s * epsilon\n",
    "            dist_lo = F.mse_loss(z_e, e_lo)\n",
    "            tries -= 1\n",
    "\n",
    "        update_vectors[ei.item()].append(e_lo)\n",
    "\n",
    "    # Token Update: average e_lo vectors for each token\n",
    "    with torch.no_grad():\n",
    "        for k in underutilized_tokens:\n",
    "            if update_vectors[k]:\n",
    "                update_vectors_tensor = torch.stack(update_vectors[k], dim=0)\n",
    "                model.vector_quantization.embedding.weight[k] = update_vectors_tensor.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved in ./results/vqvae_c:\\Users\\minju\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3ffba3fba43bd6ecf806acc7c263efc02934c74fb.json.pth\n",
      "[Epoch 0] Step 0 | Recon: 0.3512 | Loss: 0.4047 | Perplexity: 128.82\n",
      "[Epoch 0] Step 50 | Recon: 0.4549 | Loss: 0.5277 | Perplexity: 125.54\n",
      "[Epoch 0] Step 100 | Recon: 0.3519 | Loss: 0.4029 | Perplexity: 134.65\n",
      "[Epoch 0] Step 150 | Recon: 0.3677 | Loss: 0.4165 | Perplexity: 140.45\n",
      "[Epoch 0] Step 200 | Recon: 0.4330 | Loss: 0.4830 | Perplexity: 129.70\n",
      "[Epoch 0] Step 250 | Recon: 0.3364 | Loss: 0.3822 | Perplexity: 134.42\n",
      "[Epoch 0] Step 300 | Recon: 0.3200 | Loss: 0.3689 | Perplexity: 134.73\n",
      "[Epoch 0] Step 350 | Recon: 0.3386 | Loss: 0.3869 | Perplexity: 134.04\n",
      "[Epoch 0] Step 400 | Recon: 0.3112 | Loss: 0.3554 | Perplexity: 131.01\n",
      "[Epoch 0] Step 450 | Recon: 0.3769 | Loss: 0.4281 | Perplexity: 138.06\n",
      "[Epoch 0] Step 500 | Recon: 0.3240 | Loss: 0.3696 | Perplexity: 138.61\n",
      "[Epoch 0] Step 550 | Recon: 0.3323 | Loss: 0.3767 | Perplexity: 128.22\n",
      "[Epoch 0] Step 600 | Recon: 0.3409 | Loss: 0.3878 | Perplexity: 131.48\n",
      "[Epoch 0] Step 650 | Recon: 0.3072 | Loss: 0.3540 | Perplexity: 129.61\n",
      "[Epoch 0] Step 700 | Recon: 0.3509 | Loss: 0.4053 | Perplexity: 144.35\n",
      "[Epoch 0] Step 750 | Recon: 0.3176 | Loss: 0.3723 | Perplexity: 128.46\n",
      "[Epoch 0] Step 800 | Recon: 0.3236 | Loss: 0.3662 | Perplexity: 133.58\n",
      "[Epoch 0] Step 850 | Recon: 0.3364 | Loss: 0.3820 | Perplexity: 133.66\n",
      "[Epoch 0] Step 900 | Recon: 0.3777 | Loss: 0.4309 | Perplexity: 139.18\n",
      "[Epoch 0] Step 950 | Recon: 0.3662 | Loss: 0.4221 | Perplexity: 135.98\n",
      "[Epoch 0] Step 1000 | Recon: 0.3004 | Loss: 0.3453 | Perplexity: 134.49\n",
      "[Epoch 0] Step 1050 | Recon: 0.3294 | Loss: 0.3823 | Perplexity: 137.84\n",
      "[Epoch 0] Step 1100 | Recon: 0.3938 | Loss: 0.4573 | Perplexity: 139.27\n",
      "[Epoch 0] Step 1150 | Recon: 0.3488 | Loss: 0.4133 | Perplexity: 139.94\n",
      "[Epoch 0] Step 1200 | Recon: 0.2990 | Loss: 0.3598 | Perplexity: 138.37\n",
      "[Epoch 0] Step 1250 | Recon: 0.2728 | Loss: 0.3254 | Perplexity: 119.66\n",
      "[Epoch 0] Step 1300 | Recon: 0.2964 | Loss: 0.3473 | Perplexity: 139.69\n",
      "[Epoch 0] Step 1350 | Recon: 0.3609 | Loss: 0.4413 | Perplexity: 135.18\n",
      "[Epoch 0] Step 1400 | Recon: 0.3131 | Loss: 0.3749 | Perplexity: 143.46\n",
      "[Epoch 0] Step 1450 | Recon: 0.3358 | Loss: 0.4109 | Perplexity: 141.50\n",
      "[Epoch 0] Step 1500 | Recon: 0.3400 | Loss: 0.4094 | Perplexity: 139.39\n",
      "[Epoch 0] Step 1550 | Recon: 0.2660 | Loss: 0.3232 | Perplexity: 140.47\n",
      "[Epoch 0] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 0] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 1] Step 1600 | Recon: 0.3408 | Loss: 0.4145 | Perplexity: 147.91\n",
      "[Epoch 1] Step 1650 | Recon: 0.2813 | Loss: 0.3584 | Perplexity: 141.78\n",
      "[Epoch 1] Step 1700 | Recon: 0.3112 | Loss: 0.3900 | Perplexity: 142.73\n",
      "[Epoch 1] Step 1750 | Recon: 0.3158 | Loss: 0.3938 | Perplexity: 142.33\n",
      "[Epoch 1] Step 1800 | Recon: 0.3552 | Loss: 0.4497 | Perplexity: 143.04\n",
      "[Epoch 1] Step 1850 | Recon: 0.2748 | Loss: 0.3488 | Perplexity: 138.14\n",
      "[Epoch 1] Step 1900 | Recon: 0.3511 | Loss: 0.4411 | Perplexity: 151.39\n",
      "[Epoch 1] Step 1950 | Recon: 0.3057 | Loss: 0.3870 | Perplexity: 140.92\n",
      "[Epoch 1] Step 2000 | Recon: 0.3054 | Loss: 0.3821 | Perplexity: 140.49\n",
      "[Epoch 1] Step 2050 | Recon: 0.3570 | Loss: 0.4561 | Perplexity: 149.77\n",
      "[Epoch 1] Step 2100 | Recon: 0.3165 | Loss: 0.4060 | Perplexity: 150.71\n",
      "[Epoch 1] Step 2150 | Recon: 0.2969 | Loss: 0.3749 | Perplexity: 133.73\n",
      "[Epoch 1] Step 2200 | Recon: 0.2888 | Loss: 0.3750 | Perplexity: 133.78\n",
      "[Epoch 1] Step 2250 | Recon: 0.2987 | Loss: 0.3901 | Perplexity: 146.28\n",
      "[Epoch 1] Step 2300 | Recon: 0.3147 | Loss: 0.4073 | Perplexity: 145.55\n",
      "[Epoch 1] Step 2350 | Recon: 0.3012 | Loss: 0.3842 | Perplexity: 135.30\n",
      "[Epoch 1] Step 2400 | Recon: 0.3415 | Loss: 0.4462 | Perplexity: 143.53\n",
      "[Epoch 1] Step 2450 | Recon: 0.3293 | Loss: 0.4231 | Perplexity: 150.23\n",
      "[Epoch 1] Step 2500 | Recon: 0.3450 | Loss: 0.4584 | Perplexity: 144.40\n",
      "[Epoch 1] Step 2550 | Recon: 0.3162 | Loss: 0.4034 | Perplexity: 145.92\n",
      "[Epoch 1] Step 2600 | Recon: 0.3186 | Loss: 0.4138 | Perplexity: 146.80\n",
      "[Epoch 1] Step 2650 | Recon: 0.2934 | Loss: 0.3825 | Perplexity: 148.29\n",
      "[Epoch 1] Step 2700 | Recon: 0.2969 | Loss: 0.3913 | Perplexity: 156.55\n",
      "[Epoch 1] Step 2750 | Recon: 0.2647 | Loss: 0.3473 | Perplexity: 146.16\n",
      "[Epoch 1] Step 2800 | Recon: 0.2997 | Loss: 0.3982 | Perplexity: 134.63\n",
      "[Epoch 1] Step 2850 | Recon: 0.2816 | Loss: 0.3615 | Perplexity: 136.62\n",
      "[Epoch 1] Step 2900 | Recon: 0.3432 | Loss: 0.4436 | Perplexity: 149.00\n",
      "[Epoch 1] Step 2950 | Recon: 0.2446 | Loss: 0.3188 | Perplexity: 142.19\n",
      "[Epoch 1] Step 3000 | Recon: 0.3383 | Loss: 0.4506 | Perplexity: 152.13\n",
      "[Epoch 1] Step 3050 | Recon: 0.3008 | Loss: 0.4043 | Perplexity: 151.25\n",
      "[Epoch 1] Step 3100 | Recon: 0.2967 | Loss: 0.3895 | Perplexity: 145.56\n",
      "[Epoch 1] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 1] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 2] Step 3150 | Recon: 0.2971 | Loss: 0.3893 | Perplexity: 149.73\n",
      "[Epoch 2] Step 3200 | Recon: 0.3157 | Loss: 0.4150 | Perplexity: 154.11\n",
      "[Epoch 2] Step 3250 | Recon: 0.3110 | Loss: 0.4138 | Perplexity: 145.50\n",
      "[Epoch 2] Step 3300 | Recon: 0.3124 | Loss: 0.3983 | Perplexity: 148.46\n",
      "[Epoch 2] Step 3350 | Recon: 0.2987 | Loss: 0.3893 | Perplexity: 152.00\n",
      "[Epoch 2] Step 3400 | Recon: 0.3194 | Loss: 0.4265 | Perplexity: 154.03\n",
      "[Epoch 2] Step 3450 | Recon: 0.2864 | Loss: 0.3673 | Perplexity: 155.97\n",
      "[Epoch 2] Step 3500 | Recon: 0.2726 | Loss: 0.3679 | Perplexity: 151.42\n",
      "[Epoch 2] Step 3550 | Recon: 0.2812 | Loss: 0.3716 | Perplexity: 152.95\n",
      "[Epoch 2] Step 3600 | Recon: 0.2587 | Loss: 0.3476 | Perplexity: 138.91\n",
      "[Epoch 2] Step 3650 | Recon: 0.3422 | Loss: 0.4398 | Perplexity: 154.65\n",
      "[Epoch 2] Step 3700 | Recon: 0.3255 | Loss: 0.4246 | Perplexity: 152.37\n",
      "[Epoch 2] Step 3750 | Recon: 0.3054 | Loss: 0.3935 | Perplexity: 153.64\n",
      "[Epoch 2] Step 3800 | Recon: 0.3001 | Loss: 0.3846 | Perplexity: 154.40\n",
      "[Epoch 2] Step 3850 | Recon: 0.3079 | Loss: 0.4037 | Perplexity: 158.58\n",
      "[Epoch 2] Step 3900 | Recon: 0.3409 | Loss: 0.4521 | Perplexity: 155.32\n",
      "[Epoch 2] Step 3950 | Recon: 0.2842 | Loss: 0.3714 | Perplexity: 154.05\n",
      "[Epoch 2] Step 4000 | Recon: 0.2967 | Loss: 0.3944 | Perplexity: 146.91\n",
      "[Epoch 2] Step 4050 | Recon: 0.2973 | Loss: 0.3942 | Perplexity: 158.48\n",
      "[Epoch 2] Step 4100 | Recon: 0.2728 | Loss: 0.3656 | Perplexity: 148.17\n",
      "[Epoch 2] Step 4150 | Recon: 0.3170 | Loss: 0.4292 | Perplexity: 153.06\n",
      "[Epoch 2] Step 4200 | Recon: 0.3109 | Loss: 0.4369 | Perplexity: 157.48\n",
      "[Epoch 2] Step 4250 | Recon: 0.3225 | Loss: 0.4175 | Perplexity: 157.39\n",
      "[Epoch 2] Step 4300 | Recon: 0.3007 | Loss: 0.3966 | Perplexity: 151.59\n",
      "[Epoch 2] Step 4350 | Recon: 0.2987 | Loss: 0.3903 | Perplexity: 153.69\n",
      "[Epoch 2] Step 4400 | Recon: 0.2822 | Loss: 0.3865 | Perplexity: 153.41\n",
      "[Epoch 2] Step 4450 | Recon: 0.2864 | Loss: 0.3726 | Perplexity: 157.03\n",
      "[Epoch 2] Step 4500 | Recon: 0.3412 | Loss: 0.4387 | Perplexity: 161.47\n",
      "[Epoch 2] Step 4550 | Recon: 0.2676 | Loss: 0.3593 | Perplexity: 152.64\n",
      "[Epoch 2] Step 4600 | Recon: 0.2693 | Loss: 0.3504 | Perplexity: 149.50\n",
      "[Epoch 2] Step 4650 | Recon: 0.2734 | Loss: 0.3592 | Perplexity: 154.20\n",
      "[Epoch 2] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 2] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 3] Step 4700 | Recon: 0.2400 | Loss: 0.3225 | Perplexity: 142.05\n",
      "[Epoch 3] Step 4750 | Recon: 0.2805 | Loss: 0.3752 | Perplexity: 149.34\n",
      "[Epoch 3] Step 4800 | Recon: 0.3368 | Loss: 0.4490 | Perplexity: 163.24\n",
      "[Epoch 3] Step 4850 | Recon: 0.2712 | Loss: 0.3756 | Perplexity: 156.76\n",
      "[Epoch 3] Step 4900 | Recon: 0.2893 | Loss: 0.4038 | Perplexity: 157.32\n",
      "[Epoch 3] Step 4950 | Recon: 0.2884 | Loss: 0.3906 | Perplexity: 158.84\n",
      "[Epoch 3] Step 5000 | Recon: 0.2929 | Loss: 0.3924 | Perplexity: 155.44\n",
      "[Epoch 3] Step 5050 | Recon: 0.3152 | Loss: 0.4074 | Perplexity: 158.16\n",
      "[Epoch 3] Step 5100 | Recon: 0.3109 | Loss: 0.4198 | Perplexity: 153.35\n",
      "[Epoch 3] Step 5150 | Recon: 0.2719 | Loss: 0.3550 | Perplexity: 152.92\n",
      "[Epoch 3] Step 5200 | Recon: 0.3212 | Loss: 0.4156 | Perplexity: 168.02\n",
      "[Epoch 3] Step 5250 | Recon: 0.2983 | Loss: 0.3912 | Perplexity: 158.77\n",
      "[Epoch 3] Step 5300 | Recon: 0.3129 | Loss: 0.4129 | Perplexity: 157.99\n",
      "[Epoch 3] Step 5350 | Recon: 0.3801 | Loss: 0.5548 | Perplexity: 162.36\n",
      "[Epoch 3] Step 5400 | Recon: 0.2757 | Loss: 0.3634 | Perplexity: 165.28\n",
      "[Epoch 3] Step 5450 | Recon: 0.2773 | Loss: 0.3612 | Perplexity: 163.59\n",
      "[Epoch 3] Step 5500 | Recon: 0.2909 | Loss: 0.3787 | Perplexity: 165.80\n",
      "[Epoch 3] Step 5550 | Recon: 0.2671 | Loss: 0.3557 | Perplexity: 151.03\n",
      "[Epoch 3] Step 5600 | Recon: 0.3203 | Loss: 0.4277 | Perplexity: 161.93\n",
      "[Epoch 3] Step 5650 | Recon: 0.2832 | Loss: 0.3725 | Perplexity: 156.92\n",
      "[Epoch 3] Step 5700 | Recon: 0.2025 | Loss: 0.2811 | Perplexity: 139.83\n",
      "[Epoch 3] Step 5750 | Recon: 0.2588 | Loss: 0.3576 | Perplexity: 160.03\n",
      "[Epoch 3] Step 5800 | Recon: 0.2878 | Loss: 0.3816 | Perplexity: 162.44\n",
      "[Epoch 3] Step 5850 | Recon: 0.2639 | Loss: 0.3516 | Perplexity: 161.49\n",
      "[Epoch 3] Step 5900 | Recon: 0.3066 | Loss: 0.3990 | Perplexity: 166.31\n",
      "[Epoch 3] Step 5950 | Recon: 0.2933 | Loss: 0.3883 | Perplexity: 163.02\n",
      "[Epoch 3] Step 6000 | Recon: 0.2795 | Loss: 0.3828 | Perplexity: 168.59\n",
      "[Epoch 3] Step 6050 | Recon: 0.3260 | Loss: 0.4381 | Perplexity: 168.11\n",
      "[Epoch 3] Step 6100 | Recon: 0.2946 | Loss: 0.3925 | Perplexity: 161.95\n",
      "[Epoch 3] Step 6150 | Recon: 0.2621 | Loss: 0.3590 | Perplexity: 151.03\n",
      "[Epoch 3] Step 6200 | Recon: 0.2647 | Loss: 0.3532 | Perplexity: 157.13\n",
      "[Epoch 3] Step 6250 | Recon: 0.2896 | Loss: 0.3925 | Perplexity: 159.51\n",
      "[Epoch 3] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 3] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 4] Step 6300 | Recon: 0.2899 | Loss: 0.3862 | Perplexity: 162.24\n",
      "[Epoch 4] Step 6350 | Recon: 0.2845 | Loss: 0.3796 | Perplexity: 157.87\n",
      "[Epoch 4] Step 6400 | Recon: 0.2762 | Loss: 0.3837 | Perplexity: 160.20\n",
      "[Epoch 4] Step 6450 | Recon: 0.2753 | Loss: 0.3782 | Perplexity: 163.43\n",
      "[Epoch 4] Step 6500 | Recon: 0.2722 | Loss: 0.3726 | Perplexity: 163.61\n",
      "[Epoch 4] Step 6550 | Recon: 0.2865 | Loss: 0.3838 | Perplexity: 160.34\n",
      "[Epoch 4] Step 6600 | Recon: 0.2929 | Loss: 0.4006 | Perplexity: 165.12\n",
      "[Epoch 4] Step 6650 | Recon: 0.2553 | Loss: 0.3472 | Perplexity: 161.14\n",
      "[Epoch 4] Step 6700 | Recon: 0.2556 | Loss: 0.3521 | Perplexity: 163.31\n",
      "[Epoch 4] Step 6750 | Recon: 0.2714 | Loss: 0.3869 | Perplexity: 156.78\n",
      "[Epoch 4] Step 6800 | Recon: 0.3341 | Loss: 0.4553 | Perplexity: 164.23\n",
      "[Epoch 4] Step 6850 | Recon: 0.3089 | Loss: 0.4236 | Perplexity: 164.15\n",
      "[Epoch 4] Step 6900 | Recon: 0.3158 | Loss: 0.4295 | Perplexity: 169.79\n",
      "[Epoch 4] Step 6950 | Recon: 0.2722 | Loss: 0.3689 | Perplexity: 150.79\n",
      "[Epoch 4] Step 7000 | Recon: 0.2885 | Loss: 0.4015 | Perplexity: 158.46\n",
      "[Epoch 4] Step 7050 | Recon: 0.2807 | Loss: 0.3902 | Perplexity: 166.91\n",
      "[Epoch 4] Step 7100 | Recon: 0.2914 | Loss: 0.4153 | Perplexity: 164.35\n",
      "[Epoch 4] Step 7150 | Recon: 0.2970 | Loss: 0.4204 | Perplexity: 166.08\n",
      "[Epoch 4] Step 7200 | Recon: 0.2783 | Loss: 0.3877 | Perplexity: 161.08\n",
      "[Epoch 4] Step 7250 | Recon: 0.3133 | Loss: 0.4398 | Perplexity: 170.55\n",
      "[Epoch 4] Step 7300 | Recon: 0.3192 | Loss: 0.4408 | Perplexity: 172.08\n",
      "[Epoch 4] Step 7350 | Recon: 0.2832 | Loss: 0.3983 | Perplexity: 173.22\n",
      "[Epoch 4] Step 7400 | Recon: 0.2652 | Loss: 0.3708 | Perplexity: 157.75\n",
      "[Epoch 4] Step 7450 | Recon: 0.2898 | Loss: 0.4078 | Perplexity: 161.99\n",
      "[Epoch 4] Step 7500 | Recon: 0.2853 | Loss: 0.3931 | Perplexity: 164.40\n",
      "[Epoch 4] Step 7550 | Recon: 0.3259 | Loss: 0.4539 | Perplexity: 166.16\n",
      "[Epoch 4] Step 7600 | Recon: 0.2970 | Loss: 0.4125 | Perplexity: 171.37\n",
      "[Epoch 4] Step 7650 | Recon: 0.3140 | Loss: 0.4308 | Perplexity: 168.41\n",
      "[Epoch 4] Step 7700 | Recon: 0.2832 | Loss: 0.4092 | Perplexity: 161.94\n",
      "[Epoch 4] Step 7750 | Recon: 0.2574 | Loss: 0.3569 | Perplexity: 176.15\n",
      "[Epoch 4] Step 7800 | Recon: 0.2913 | Loss: 0.4027 | Perplexity: 173.23\n",
      "[Epoch 4] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 4] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 5] Step 7850 | Recon: 0.2889 | Loss: 0.4185 | Perplexity: 168.88\n",
      "[Epoch 5] Step 7900 | Recon: 0.2356 | Loss: 0.3373 | Perplexity: 155.53\n",
      "[Epoch 5] Step 7950 | Recon: 0.2833 | Loss: 0.4001 | Perplexity: 163.13\n",
      "[Epoch 5] Step 8000 | Recon: 0.2808 | Loss: 0.3907 | Perplexity: 166.88\n",
      "[Epoch 5] Step 8050 | Recon: 0.3273 | Loss: 0.4697 | Perplexity: 170.12\n",
      "[Epoch 5] Step 8100 | Recon: 0.2806 | Loss: 0.3854 | Perplexity: 168.56\n",
      "[Epoch 5] Step 8150 | Recon: 0.2613 | Loss: 0.3723 | Perplexity: 161.56\n",
      "[Epoch 5] Step 8200 | Recon: 0.2909 | Loss: 0.4083 | Perplexity: 169.99\n",
      "[Epoch 5] Step 8250 | Recon: 0.3160 | Loss: 0.4493 | Perplexity: 170.95\n",
      "[Epoch 5] Step 8300 | Recon: 0.3061 | Loss: 0.4390 | Perplexity: 168.07\n",
      "[Epoch 5] Step 8350 | Recon: 0.2601 | Loss: 0.3629 | Perplexity: 158.43\n",
      "[Epoch 5] Step 8400 | Recon: 0.3322 | Loss: 0.4828 | Perplexity: 174.91\n",
      "[Epoch 5] Step 8450 | Recon: 0.2961 | Loss: 0.4211 | Perplexity: 174.23\n",
      "[Epoch 5] Step 8500 | Recon: 0.2879 | Loss: 0.4182 | Perplexity: 176.87\n",
      "[Epoch 5] Step 8550 | Recon: 0.2604 | Loss: 0.3610 | Perplexity: 168.61\n",
      "[Epoch 5] Step 8600 | Recon: 0.2830 | Loss: 0.3965 | Perplexity: 160.22\n",
      "[Epoch 5] Step 8650 | Recon: 0.2930 | Loss: 0.4206 | Perplexity: 169.33\n",
      "[Epoch 5] Step 8700 | Recon: 0.2832 | Loss: 0.3950 | Perplexity: 167.70\n",
      "[Epoch 5] Step 8750 | Recon: 0.2957 | Loss: 0.4150 | Perplexity: 174.07\n",
      "[Epoch 5] Step 8800 | Recon: 0.2672 | Loss: 0.3870 | Perplexity: 174.46\n",
      "[Epoch 5] Step 8850 | Recon: 0.2589 | Loss: 0.3728 | Perplexity: 172.11\n",
      "[Epoch 5] Step 8900 | Recon: 0.2993 | Loss: 0.4234 | Perplexity: 172.97\n",
      "[Epoch 5] Step 8950 | Recon: 0.3073 | Loss: 0.4507 | Perplexity: 177.50\n",
      "[Epoch 5] Step 9000 | Recon: 0.2332 | Loss: 0.3287 | Perplexity: 167.85\n",
      "[Epoch 5] Step 9050 | Recon: 0.2754 | Loss: 0.3840 | Perplexity: 171.49\n",
      "[Epoch 5] Step 9100 | Recon: 0.2745 | Loss: 0.3893 | Perplexity: 171.28\n",
      "[Epoch 5] Step 9150 | Recon: 0.3203 | Loss: 0.4542 | Perplexity: 176.04\n",
      "[Epoch 5] Step 9200 | Recon: 0.3023 | Loss: 0.4367 | Perplexity: 171.34\n",
      "[Epoch 5] Step 9250 | Recon: 0.2901 | Loss: 0.4279 | Perplexity: 166.01\n",
      "[Epoch 5] Step 9300 | Recon: 0.2383 | Loss: 0.3588 | Perplexity: 166.09\n",
      "[Epoch 5] Step 9350 | Recon: 0.2901 | Loss: 0.4128 | Perplexity: 175.23\n",
      "[Epoch 5] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 5] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 6] Step 9400 | Recon: 0.3299 | Loss: 0.4885 | Perplexity: 168.61\n",
      "[Epoch 6] Step 9450 | Recon: 0.2861 | Loss: 0.4098 | Perplexity: 158.89\n",
      "[Epoch 6] Step 9500 | Recon: 0.3224 | Loss: 0.4623 | Perplexity: 177.40\n",
      "[Epoch 6] Step 9550 | Recon: 0.3248 | Loss: 0.4599 | Perplexity: 167.96\n",
      "[Epoch 6] Step 9600 | Recon: 0.2956 | Loss: 0.4206 | Perplexity: 165.25\n",
      "[Epoch 6] Step 9650 | Recon: 0.2510 | Loss: 0.3643 | Perplexity: 169.04\n",
      "[Epoch 6] Step 9700 | Recon: 0.2657 | Loss: 0.3891 | Perplexity: 172.76\n",
      "[Epoch 6] Step 9750 | Recon: 0.2926 | Loss: 0.4299 | Perplexity: 169.96\n",
      "[Epoch 6] Step 9800 | Recon: 0.3276 | Loss: 0.4601 | Perplexity: 169.19\n",
      "[Epoch 6] Step 9850 | Recon: 0.2695 | Loss: 0.3993 | Perplexity: 169.45\n",
      "[Epoch 6] Step 9900 | Recon: 0.2809 | Loss: 0.4032 | Perplexity: 165.89\n",
      "[Epoch 6] Step 9950 | Recon: 0.2564 | Loss: 0.3690 | Perplexity: 171.64\n",
      "[Epoch 6] Step 10000 | Recon: 0.2941 | Loss: 0.4182 | Perplexity: 167.85\n",
      "[Epoch 6] Step 10050 | Recon: 0.2728 | Loss: 0.3880 | Perplexity: 165.92\n",
      "[Epoch 6] Step 10100 | Recon: 0.2911 | Loss: 0.4228 | Perplexity: 170.35\n",
      "[Epoch 6] Step 10150 | Recon: 0.2684 | Loss: 0.3841 | Perplexity: 172.38\n",
      "[Epoch 6] Step 10200 | Recon: 0.2662 | Loss: 0.3848 | Perplexity: 166.19\n",
      "[Epoch 6] Step 10250 | Recon: 0.2353 | Loss: 0.3390 | Perplexity: 154.99\n",
      "[Epoch 6] Step 10300 | Recon: 0.2642 | Loss: 0.3789 | Perplexity: 167.86\n",
      "[Epoch 6] Step 10350 | Recon: 0.2935 | Loss: 0.4170 | Perplexity: 175.52\n",
      "[Epoch 6] Step 10400 | Recon: 0.3059 | Loss: 0.4361 | Perplexity: 174.19\n",
      "[Epoch 6] Step 10450 | Recon: 0.2793 | Loss: 0.4068 | Perplexity: 170.17\n",
      "[Epoch 6] Step 10500 | Recon: 0.2395 | Loss: 0.3486 | Perplexity: 171.32\n",
      "[Epoch 6] Step 10550 | Recon: 0.2864 | Loss: 0.4124 | Perplexity: 169.45\n",
      "[Epoch 6] Step 10600 | Recon: 0.3007 | Loss: 0.4353 | Perplexity: 172.24\n",
      "[Epoch 6] Step 10650 | Recon: 0.2707 | Loss: 0.3998 | Perplexity: 168.53\n",
      "[Epoch 6] Step 10700 | Recon: 0.2542 | Loss: 0.3806 | Perplexity: 162.40\n",
      "[Epoch 6] Step 10750 | Recon: 0.2731 | Loss: 0.4048 | Perplexity: 171.00\n",
      "[Epoch 6] Step 10800 | Recon: 0.2729 | Loss: 0.4008 | Perplexity: 170.15\n",
      "[Epoch 6] Step 10850 | Recon: 0.2178 | Loss: 0.3212 | Perplexity: 159.17\n",
      "[Epoch 6] Step 10900 | Recon: 0.2445 | Loss: 0.3635 | Perplexity: 166.94\n",
      "[Epoch 6] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 6] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 7] Step 10950 | Recon: 0.2271 | Loss: 0.3389 | Perplexity: 157.18\n",
      "[Epoch 7] Step 11000 | Recon: 0.2921 | Loss: 0.4238 | Perplexity: 166.00\n",
      "[Epoch 7] Step 11050 | Recon: 0.3021 | Loss: 0.4571 | Perplexity: 174.91\n",
      "[Epoch 7] Step 11100 | Recon: 0.2932 | Loss: 0.4604 | Perplexity: 174.05\n",
      "[Epoch 7] Step 11150 | Recon: 0.2883 | Loss: 0.4372 | Perplexity: 167.77\n",
      "[Epoch 7] Step 11200 | Recon: 0.2628 | Loss: 0.3957 | Perplexity: 160.64\n",
      "[Epoch 7] Step 11250 | Recon: 0.2445 | Loss: 0.3703 | Perplexity: 160.86\n",
      "[Epoch 7] Step 11300 | Recon: 0.2398 | Loss: 0.3468 | Perplexity: 164.51\n",
      "[Epoch 7] Step 11350 | Recon: 0.2691 | Loss: 0.4021 | Perplexity: 167.72\n",
      "[Epoch 7] Step 11400 | Recon: 0.2462 | Loss: 0.3683 | Perplexity: 163.52\n",
      "[Epoch 7] Step 11450 | Recon: 0.2695 | Loss: 0.4059 | Perplexity: 151.94\n",
      "[Epoch 7] Step 11500 | Recon: 0.2918 | Loss: 0.4169 | Perplexity: 167.88\n",
      "[Epoch 7] Step 11550 | Recon: 0.2457 | Loss: 0.3588 | Perplexity: 162.96\n",
      "[Epoch 7] Step 11600 | Recon: 0.2655 | Loss: 0.3968 | Perplexity: 168.54\n",
      "[Epoch 7] Step 11650 | Recon: 0.2826 | Loss: 0.4239 | Perplexity: 167.91\n",
      "[Epoch 7] Step 11700 | Recon: 0.2735 | Loss: 0.3926 | Perplexity: 168.33\n",
      "[Epoch 7] Step 11750 | Recon: 0.2568 | Loss: 0.3809 | Perplexity: 163.28\n",
      "[Epoch 7] Step 11800 | Recon: 0.2276 | Loss: 0.3375 | Perplexity: 160.33\n",
      "[Epoch 7] Step 11850 | Recon: 0.2487 | Loss: 0.3719 | Perplexity: 165.93\n",
      "[Epoch 7] Step 11900 | Recon: 0.3088 | Loss: 0.4566 | Perplexity: 174.27\n",
      "[Epoch 7] Step 11950 | Recon: 0.2260 | Loss: 0.3313 | Perplexity: 156.87\n",
      "[Epoch 7] Step 12000 | Recon: 0.2734 | Loss: 0.4045 | Perplexity: 170.97\n",
      "[Epoch 7] Step 12050 | Recon: 0.2619 | Loss: 0.3837 | Perplexity: 166.26\n",
      "[Epoch 7] Step 12100 | Recon: 0.2698 | Loss: 0.3975 | Perplexity: 163.58\n",
      "[Epoch 7] Step 12150 | Recon: 0.2427 | Loss: 0.3638 | Perplexity: 162.55\n",
      "[Epoch 7] Step 12200 | Recon: 0.2722 | Loss: 0.3967 | Perplexity: 168.59\n",
      "[Epoch 7] Step 12250 | Recon: 0.2376 | Loss: 0.3552 | Perplexity: 167.76\n",
      "[Epoch 7] Step 12300 | Recon: 0.2715 | Loss: 0.3985 | Perplexity: 174.60\n",
      "[Epoch 7] Step 12350 | Recon: 0.2754 | Loss: 0.4048 | Perplexity: 167.12\n",
      "[Epoch 7] Step 12400 | Recon: 0.2812 | Loss: 0.4297 | Perplexity: 159.92\n",
      "[Epoch 7] Step 12450 | Recon: 0.3043 | Loss: 0.4382 | Perplexity: 177.25\n",
      "[Epoch 7] Step 12500 | Recon: 0.2480 | Loss: 0.3782 | Perplexity: 163.85\n",
      "[Epoch 7] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 7] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 8] Step 12550 | Recon: 0.2782 | Loss: 0.4215 | Perplexity: 168.24\n",
      "[Epoch 8] Step 12600 | Recon: 0.2574 | Loss: 0.3915 | Perplexity: 157.33\n",
      "[Epoch 8] Step 12650 | Recon: 0.2816 | Loss: 0.4208 | Perplexity: 169.47\n",
      "[Epoch 8] Step 12700 | Recon: 0.2752 | Loss: 0.4037 | Perplexity: 174.96\n",
      "[Epoch 8] Step 12750 | Recon: 0.2626 | Loss: 0.3913 | Perplexity: 171.68\n",
      "[Epoch 8] Step 12800 | Recon: 0.2775 | Loss: 0.4009 | Perplexity: 166.40\n",
      "[Epoch 8] Step 12850 | Recon: 0.2628 | Loss: 0.3940 | Perplexity: 163.13\n",
      "[Epoch 8] Step 12900 | Recon: 0.2546 | Loss: 0.3926 | Perplexity: 162.28\n",
      "[Epoch 8] Step 12950 | Recon: 0.2701 | Loss: 0.4098 | Perplexity: 169.91\n",
      "[Epoch 8] Step 13000 | Recon: 0.2538 | Loss: 0.3809 | Perplexity: 165.97\n",
      "[Epoch 8] Step 13050 | Recon: 0.2730 | Loss: 0.4021 | Perplexity: 169.09\n",
      "[Epoch 8] Step 13100 | Recon: 0.2871 | Loss: 0.4219 | Perplexity: 165.76\n",
      "[Epoch 8] Step 13150 | Recon: 0.2986 | Loss: 0.4500 | Perplexity: 161.38\n",
      "[Epoch 8] Step 13200 | Recon: 0.2755 | Loss: 0.4054 | Perplexity: 174.51\n",
      "[Epoch 8] Step 13250 | Recon: 0.2465 | Loss: 0.3735 | Perplexity: 162.79\n",
      "[Epoch 8] Step 13300 | Recon: 0.2321 | Loss: 0.3514 | Perplexity: 163.09\n",
      "[Epoch 8] Step 13350 | Recon: 0.2566 | Loss: 0.3860 | Perplexity: 157.73\n",
      "[Epoch 8] Step 13400 | Recon: 0.2279 | Loss: 0.3512 | Perplexity: 163.36\n",
      "[Epoch 8] Step 13450 | Recon: 0.2894 | Loss: 0.4284 | Perplexity: 170.51\n",
      "[Epoch 8] Step 13500 | Recon: 0.2291 | Loss: 0.3649 | Perplexity: 153.80\n",
      "[Epoch 8] Step 13550 | Recon: 0.2855 | Loss: 0.4149 | Perplexity: 176.46\n",
      "[Epoch 8] Step 13600 | Recon: 0.2484 | Loss: 0.3710 | Perplexity: 155.85\n",
      "[Epoch 8] Step 13650 | Recon: 0.2731 | Loss: 0.4124 | Perplexity: 164.64\n",
      "[Epoch 8] Step 13700 | Recon: 0.2604 | Loss: 0.3945 | Perplexity: 170.47\n",
      "[Epoch 8] Step 13750 | Recon: 0.2614 | Loss: 0.3837 | Perplexity: 173.16\n",
      "[Epoch 8] Step 13800 | Recon: 0.2759 | Loss: 0.4110 | Perplexity: 177.36\n",
      "[Epoch 8] Step 13850 | Recon: 0.2227 | Loss: 0.3607 | Perplexity: 160.24\n",
      "[Epoch 8] Step 13900 | Recon: 0.2587 | Loss: 0.3983 | Perplexity: 171.10\n",
      "[Epoch 8] Step 13950 | Recon: 0.2421 | Loss: 0.3625 | Perplexity: 161.90\n",
      "[Epoch 8] Step 14000 | Recon: 0.2613 | Loss: 0.3969 | Perplexity: 171.43\n",
      "[Epoch 8] Step 14050 | Recon: 0.2598 | Loss: 0.3883 | Perplexity: 170.40\n",
      "[Epoch 8] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 8] ✅ CGA update complete.\n",
      "\n",
      "[Epoch 9] Step 14100 | Recon: 0.2585 | Loss: 0.3943 | Perplexity: 164.76\n",
      "[Epoch 9] Step 14150 | Recon: 0.2509 | Loss: 0.3782 | Perplexity: 164.74\n",
      "[Epoch 9] Step 14200 | Recon: 0.2441 | Loss: 0.3741 | Perplexity: 165.66\n",
      "[Epoch 9] Step 14250 | Recon: 0.2528 | Loss: 0.3836 | Perplexity: 166.64\n",
      "[Epoch 9] Step 14300 | Recon: 0.2557 | Loss: 0.3925 | Perplexity: 169.72\n",
      "[Epoch 9] Step 14350 | Recon: 0.2466 | Loss: 0.3771 | Perplexity: 167.47\n",
      "[Epoch 9] Step 14400 | Recon: 0.2662 | Loss: 0.4039 | Perplexity: 169.73\n",
      "[Epoch 9] Step 14450 | Recon: 0.2644 | Loss: 0.4178 | Perplexity: 159.80\n",
      "[Epoch 9] Step 14500 | Recon: 0.2795 | Loss: 0.4275 | Perplexity: 166.35\n",
      "[Epoch 9] Step 14550 | Recon: 0.2768 | Loss: 0.4093 | Perplexity: 169.33\n",
      "[Epoch 9] Step 14600 | Recon: 0.2506 | Loss: 0.3876 | Perplexity: 163.19\n",
      "[Epoch 9] Step 14650 | Recon: 0.3042 | Loss: 0.4505 | Perplexity: 173.91\n",
      "[Epoch 9] Step 14700 | Recon: 0.2547 | Loss: 0.3807 | Perplexity: 161.19\n",
      "[Epoch 9] Step 14750 | Recon: 0.2554 | Loss: 0.3891 | Perplexity: 160.50\n",
      "[Epoch 9] Step 14800 | Recon: 0.2318 | Loss: 0.3462 | Perplexity: 155.23\n",
      "[Epoch 9] Step 14850 | Recon: 0.2522 | Loss: 0.3904 | Perplexity: 165.05\n",
      "[Epoch 9] Step 14900 | Recon: 0.2612 | Loss: 0.3929 | Perplexity: 170.59\n",
      "[Epoch 9] Step 14950 | Recon: 0.2544 | Loss: 0.3821 | Perplexity: 167.53\n",
      "[Epoch 9] Step 15000 | Recon: 0.2886 | Loss: 0.4304 | Perplexity: 167.22\n",
      "[Epoch 9] Step 15050 | Recon: 0.2485 | Loss: 0.3920 | Perplexity: 160.60\n",
      "[Epoch 9] Step 15100 | Recon: 0.3419 | Loss: 0.5353 | Perplexity: 167.08\n",
      "[Epoch 9] Step 15150 | Recon: 0.2555 | Loss: 0.3989 | Perplexity: 165.11\n",
      "[Epoch 9] Step 15200 | Recon: 0.2934 | Loss: 0.4335 | Perplexity: 171.24\n",
      "[Epoch 9] Step 15250 | Recon: 0.2604 | Loss: 0.3881 | Perplexity: 160.19\n",
      "[Epoch 9] Step 15300 | Recon: 0.2835 | Loss: 0.4372 | Perplexity: 172.24\n",
      "[Epoch 9] Step 15350 | Recon: 0.2546 | Loss: 0.3804 | Perplexity: 160.67\n",
      "[Epoch 9] Step 15400 | Recon: 0.2549 | Loss: 0.3921 | Perplexity: 165.68\n",
      "[Epoch 9] Step 15450 | Recon: 0.2784 | Loss: 0.4294 | Perplexity: 170.85\n",
      "[Epoch 9] Step 15500 | Recon: 0.2714 | Loss: 0.4210 | Perplexity: 169.19\n",
      "[Epoch 9] Step 15550 | Recon: 0.2723 | Loss: 0.4309 | Perplexity: 157.67\n",
      "[Epoch 9] Step 15600 | Recon: 0.2978 | Loss: 0.4526 | Perplexity: 180.57\n",
      "[Epoch 9] ⏳ Running CGA update for underutilized tokens...\n",
      "[Epoch 9] ✅ CGA update complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\"\"\"\n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "timestamp = utils.readable_timestamp()\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--n_updates\", type=int, default=10)\n",
    "parser.add_argument(\"--n_hiddens\", type=int, default=128)\n",
    "parser.add_argument(\"--n_residual_hiddens\", type=int, default=32)\n",
    "parser.add_argument(\"--n_residual_layers\", type=int, default=2)\n",
    "parser.add_argument(\"--embedding_dim\", type=int, default=64)\n",
    "parser.add_argument(\"--n_embeddings\", type=int, default=512)\n",
    "parser.add_argument(\"--beta\", type=float, default=.25)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=3e-4)\n",
    "parser.add_argument(\"--log_interval\", type=int, default=50)\n",
    "parser.add_argument(\"--dataset\",  type=str, default='CIFAR10')\n",
    "\n",
    "# whether or not to save model\n",
    "parser.add_argument(\"-save\", action=\"store_true\")\n",
    "parser.add_argument(\"--filename\",  type=str, default=timestamp)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.save = True\n",
    "if args.save:\n",
    "    print('Results will be saved in ./results/vqvae_' + args.filename + '.pth')\n",
    "\n",
    "\n",
    "training_data, validation_data, training_loader, validation_loader, x_train_var = utils.load_data_and_data_loaders(\n",
    "    args.dataset, args.batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, amsgrad=True)\n",
    "\n",
    "\n",
    "results = {\n",
    "    'n_updates': 0,\n",
    "    'recon_errors': [],\n",
    "    'loss_vals': [],\n",
    "    'perplexities': [],\n",
    "}\n",
    "def train():\n",
    "    model.train()\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(args.n_updates):\n",
    "        all_z_e = []\n",
    "        all_ei = []\n",
    "        all_ej = []\n",
    "\n",
    "        for (x, _) in training_loader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with z_e, top-1, top-2 indices\n",
    "            embedding_loss, x_hat, perplexity, z_e, ei, ej = model(x)\n",
    "            recon_loss = torch.mean((x_hat - x) ** 2) / x_train_var\n",
    "            loss = recon_loss + embedding_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 기록\n",
    "            results[\"recon_errors\"].append(recon_loss.item())\n",
    "            results[\"perplexities\"].append(perplexity.item())\n",
    "            results[\"loss_vals\"].append(loss.item())\n",
    "            results[\"n_updates\"] = step\n",
    "\n",
    "            # CGA 준비를 위한 데이터 누적\n",
    "            z_e_flat = z_e.permute(0, 2, 3, 1).reshape(-1, z_e.shape[1])  # (BHW, D)\n",
    "            all_z_e.append(z_e_flat.detach().cpu())\n",
    "            all_ei.append(ei.view(-1).detach().cpu())\n",
    "            all_ej.append(ej.view(-1).detach().cpu())\n",
    "\n",
    "            # 로그\n",
    "            if step % args.log_interval == 0:\n",
    "                print(f\"[Epoch {epoch}] Step {step} | Recon: {recon_loss.item():.4f} | \"\n",
    "                      f\"Loss: {loss.item():.4f} | Perplexity: {perplexity.item():.2f}\")\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # === ✅ 1 에폭 끝난 후 CGA 업데이트 ===\n",
    "        print(f\"[Epoch {epoch}] ⏳ Running CGA update for underutilized tokens...\")\n",
    "        z_e_all = torch.cat(all_z_e, dim=0).to(device)\n",
    "        ei_all = torch.cat(all_ei, dim=0)\n",
    "        ej_all = torch.cat(all_ej, dim=0)\n",
    "\n",
    "        usage_counts = torch.bincount(ei_all, minlength=model.vector_quantization.n_e)\n",
    "        update_codebook_with_cga(model, z_e_all, ei_all, ej_all, usage_counts)\n",
    "\n",
    "        print(f\"[Epoch {epoch}] ✅ CGA update complete.\\n\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most common representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.21421266..1.4307272].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAENCAYAAACSOWa6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQ2RJREFUeJztncuPI8d9xz8kZ8h5P7TyyivJAWQ7RlZykFwUwEgA24fYcJAHAhiIgRxzydV/hP+Q3JJLkkuQAEIsI0DiwHk4NiA4siLZ613t7Oxrdt58M4dv1UyTW+zp6WnWNKd/n0stOc1mf/gdDH9bVV1VG41GIwzDMAzDqCz1674AwzAMwzCuFysGDMMwDKPiWDFgGIZhGBXHigHDMAzDqDhWDBiGYRhGxbFiwDAMwzAqjhUDhmEYhlFxFrIcNBwOefjwIevr69RqtVlfk2EYhmEYBTAajTg8POT111+nXp/+//9MxcDDhw/53Oc+V9jFGYZhGIYRj/v37/Pmm29O/XmmYYL19fXCLsgwDMMwjLhc9D2eqRiwoQHDMAzDmF8u+h63CYSGYRiGUXEyzRkIcVP2N+q79sS1Tdf2XLuRqKZuijOMe6c5w83xtqxFE8vasr553pa16PFy1lnIXQzcFPyvRsu1A9fe9A8m6V1FZ6i2dxWdoZreN90Z5G1Zi7zeNkxgGIZhGBXnphdPU+lPtJ6Ga29OZ9I4Ie+b7gzytaxFg5vrDJZ1kipknWyh2llDfm/rGTAMwzCMilOanoERUPQNjH4MxVc8/vxd4GjimEXXrrg2xgfjK7hZeiedIewd0xniZw3yvulZ+/OnZQ3yjpm1v66isKzPz3/TswY5pWXtj7npWcNsv7usZ8AwDMMwKk5pegZmsaxRY8rzTWDb/bvj2u7EMdNeWySzWsopdO3+tpM07xjOED9rkLdlnf66WWBZF0cVs4bs3jc9a5jtd5f1DBiGYRhGxSlNz0Bshq71VdWaaweBY28SIe+b7gzytqxFFbyr6AzV9q6iMxTnHa0YaLt2yZn06y9fgL9VYlYXlfyw/Afpu0YOXbtZ8HsmvUPOMFvvyV+QkPcsneH6swZ5x8wa5B0za4j/O54la5C3ZV0s1/X3rGxZg7xjZg0377vLhgkMwzAMo+LMvGfArw/d76qeOWlo6kdjqMfDuh4P+rA846vxEyvawKn7t781xN+W4dd59rdr5CXkHXKG2XonnSHsXZQzyLtsWYO8Y2YN8o6ZNcg7LWuQd8ysIf7vuGUtiswa5F22rEHeMbMGecfMGmb73WU9A4ZhGIZRcWbeM3B0qDqm46qpQU9LJtQWVWVtDlXHLG7P6iaNc/xYS4dzcV9dLU55nJeQ93U7Q9i7KGeQd9myBnnHzBrkHdMZ5J2WtX9sWeejylmDvMuWNYx7xsga5B0za5jtd5f1DBiGYRhGxZlpz8DeqE23oUGV4Y6qrJNljfYsHKjWeeE2YOysLHCn9QYAyzO6nmai9eMukxXmVdkbyS/kHXIGZurdnGhjeJcta5B3zKxB3jGzBvmWLWuQt2VdLLGzBnmXLWsY/x/zVcmSNcg7Zta+nVXW1jNgGIZhGBUnf1ExPGbkSomRG6wY1VQxNdw0zFa/BUNVU8e3jvWzR/sAtE81P3Jv+SkA9Y/q1Db0urd+7Y3cl5UVP67i7x3t1cefDzKUQ8h7zBmC3iFn4Nq9LxxjSninZg0wbJcua5BjzKxB3mXLOvl8kBlkDfKex6xB3jc9a5B3WtYg77JlDfKOmTXIe26yzoD1DBiGYRhGxamNRr42ms7BwQGbm+PrGz168DN6PY1erDRfBaC1rNqiuarK6WTUgD3Nthy2dRdkzQ1+vHAzM3cf3Afg+bM2Azdn8vU7qq7uvv0OAMurRa8pdc6xa1dd6+8t9VVWrXY+W/TRg5/pmID3mDMEvUPOQNB7ls4w7p3mDOPeqVkD7NUs64R32bIGecfMGuQ9j1mDvIvKGuJ7Z8ka5J2WNci7bFmDvGNmDfIuW9Yg78msAfb399nY2Jh6vtzDBMPjYw4a+kCXu0sA1Ne2AKg1dCGb/TWGW6o16u4DHvT1oR0/1/SHtpsN8eFHn3D/010AOr33AfjyO18F4Ju/r/atL3w27+VOxX+Qfr3ntK6W4bE+/pB30hkIeoecgaD3LJ1h3Pui7qWkd1rWIG/L+ty7bFlDdu+isgZ5z2PWIO+isgZ5ly1rkHda1iDvsmUN8o6ZNch7XrLOgg0TGIZhGEbFyd0z0O0NaHdVkwxvu5GGBT8bQ10vtBqJakOLJjY6ek1npOrr6PQXOt/Ox/zne98H4MUTVW0/fv8/APjowY8A+Paff4ev3H037yWPMZhom9MOTNDt6eigd8IZCHqHnIGgd9IZKI13WtbgvcuVNcg3ZtYgb8v63DstayjWu7CsQd4FZQ3yLlvW4LxTswZYKV3Wvo2ZNch7XrLOgvUMGIZhGEbFyd0zsFTrs47GlU5qWpxh1U2iaCz4xRMDSzC0VH+0Wm6k42AHgI8/eciHH/8vALunrkrb/SUA9z/4MQDDH/0SvvddAL7y7u/lvfQx/HiLr7Ia0w5EzkDQO9UZoFUPOgNB7zFnmKl3mjOMe1vW2bIG512yrCG7d1FZg7xTswb43nev3Rmm/T0rJmuQd9myBnnPY9Yg76hZAxzszE3WWbCeAcMwDMOoOLl7Bg5r0DnRTMyFI1VS3VWdrumKyoXEYEbf/8PtPXnwTLdr7Nb1mtN2j8Gpf0GHJPdd7fNX7/0tp9s6pv2dFwB8/U//MNf1Nybak4nHrcBrDt3dGiHvkDOMe4ecgaB30hmYqXeaM4x7z2PWIMeYWYO8y5a1fxwza/De07MGeZcta5B3UVmDvMuWNcg7LWtw3iXL2rcxswZ5ly1rmJ73RVjPgGEYhmFUnNw9A8sAr6raW6qrjDwZ6nS1jqqjVndAY00zLweu0jw8fAHA48NPAWjvavbl8+0OzbNNGMPsAf/8g3/S64Y6treqa/jGu1/XQdvTF1VIw1dT/ZRjzkaSAt5JZyDoHXIGUr33XBvyvqozyDvNGca907IGeVvWLwB5ly1ryO5dVNaQ3btsWYO8Y2YNzjti1iDvtKxB3pb1ufe8ZJ2F3MVAf7lJq+OWOVjWVIYXO08AOHaTMlpbGyyc6pMcnr4A4PGhW7DhqR7X3Q5Q28NVtt9UVA8fO62unyJxzr3Hel373/4RgKOD5wD8159o8Ye/+LNvc/vW5y7t47tY0j7Q/rK6eULeSWcg6B1yBnJ7J52B3N4X/RKNeadkDfKOmTVc3jtG1iDvsmUNl/AuKGuQd5ozyLtsWYO8Y2YN8o6aNcDyMDVrkLdlrcf1g+7cZJ0FGyYwDMMwjIqTu2dgfW2DxrK6O/pL6qhoPlW3x+BEu1gttfd5sqh65WhXFdnuiR7X97Us5Oobev6Li1/g1C0Isfo/jwDY+VS3rHTbD/RaOOuMefDwAIDe838F4HlbszuWTvb41h//JQC/8aXX8+oFWV9TxRjyTjoDQe+QMxD0TjpD2DvpDETxTssa5B0za5B32bIGeVvW595pWYO8y5Y1yLuorEHeaVmDvGNmDfJOyxrkHTNrKNa7qKxB3vOSdRasZ8AwDMMwKk7+noFXtxkN1gFonKpS6iy5yRh9VXg7h8ccnehejk5XVdUrtzTZpDPQbk6rbhnI1+5s8pt3vgzAJ1/V+M3p7mMAnjzWcpcPH+2y80CLPPR3DvUefR37wQ9/CEBvcMTg6RYAT7+pCSlvv63q7ZXXpk/S8FXbwsTjSWcg6J10BoLeIWcg6J10BoLeSWcg6J3m7D3TnCe907L23jGz9t5ly9p7x8z6Mt4xs/beaVl777Jl7b2Lytp7p2XtvWNm7b3TsvbeMbP23mXL2nuXLetp3lmwngHDMAzDqDi5ewYWuQUNN39xTRs53F7SDMqDpqsxaj2OdzT2tOCfO9UGGJu3NTPzzsaXABhtt1hvqsr62qHOs9zUPMl2XbMuH917yAe/+AiAn/70QwB+8oP3APj5vf8DYP/jR/xD5+8AWOdXOs/eHwBQ/9Y3ANgKrDbpd39OW9JxkVvuoJe9k85A0DvkDAS9k85A0DvpDAS905y990XLWI55p2TtvWNm7b3LlrX3jpm1907L+lLeBWXtvdOy9t5ly/rMu6CsvXda1t47atbOOy3rM++IWXvvsmXtvWNmfRXvLFjPgGEYhmFUnNpoNLpwiOHg4IDNzc2x59JeNuhobGWvu8fzJ6rA2u74tTU9vr2uBR1Wl2/rQmq1ydOk0j1Q9fb3f/PXAHz/X94H4Fef3mOhpqUm72zpPX77j1RVfe13NQ5z99c/n/l9kteV1RkIel/VGca9k85A0PuqznB575hZg7zLljXIO2bWIG/L+mKyZg3yjpk1yLtsWYO8Y2YN8i5b1iDvmFlDdu/Qde3v77OxMX3uwUyKAb+Ic5slhh11D43QhI1+Qz/bXPD9HblHKsb4+b//NwA/uf8hj5+oy+XkULd73H3ziwC89Vu6XeOdu7+TuU8l+y/SuTMQ9J6lMxD0HnOGTN6X+aMR8o6ZNcg7NWu4tPdVswbvHS9rkLdlffF5L/MFAe2oWYO8y5Y1yDtm1iDv8mUNRXpnyRqyf3flKQZsmMAwDMMwKs6MegZC+MUS/T7ZK5d8fXa6R3qPe/c1YaOpQo/NlTcB2HptM/i6EJerKkMkvWfnDGHvqzpDubMGeZcva7iu33HL+mIs66t4x8sa5G1ZX87begYMwzAMw7g0EXsGJvFVlqqu0WEd1lUGXX5KRkaOtZjEaHU183tcvapMEnCG2XonnLO+RzH/g0hyvVlnfZ9iswZ5R8wa4Pi4fFkDrC9Z1kVjWV9j1gD9qFlD9u8u6xkwDMMwDOPS5O8ZGOxC/RX3yM+q9GMr9Yk2iRZsOEZbSA73tCnE82dP6C1qAYfNDVVZn9l+LZNEHgYHWlCiseG9fCU1Pi40VlUO3JYTQe80Z4BO0Bkogfd0Z5j0nr+sQd4xswZ5ly9rSM5fiZE1yHs+swb53vSsQd7zlzV473hZg7zLlzXAivUMGIZhGIZxeXLfKNnpLlCvvwCgsaiaouYWTKj1V146+8C1jYGqqvpACy+0m6rIet0RTx5o28f6Z3Qj5UJHP9v+7Bt5L3MqtQ0/I/TItU3X+o6S4Uuv6XQlFPIOOcO4d8gZCHrPwhmmeU93hnHvecwavHe8rEHe5csa5B0va5D3PGYNzrugrOE6/p5dnDXIOy1rkHf5sgZ5x8sa5F2+rGFa3hdhPQOGYRiGUXFy9wwcdw4ZjTQu0UIzOxcWVUMtrPbcyRu8VG809HjU0BhLY99tB7l+zGlDlc3JU1U+o7rO1z/R0phbn192ozVXp352r+jkPaOq6Pqjk5dec9zR1pMh73FnvcMYjXrQGQh6J52BGXtPd4Zx7/Ss9Q5nlCRrXdUKMbMGeZcva4B+1KxB3mlZQ9HexWQN8i4qa5B32bIGec9n1pD171lRWYO8y5Y1TP87fhG5JxCePH0Ia+4D7boPz+0AVffdFrVL6Hd7dLoHAPRG+kXsn+g8o5ZC2FrZotYMv7w4BmNtrdY6+8nJU+0+FfLO6wwEvZPOwIy9pzvDuLdlPe9Zg3zjZQ3yTssaru93PC1rcN4FZQ3yLlvW4Lwt6znPWu1k1mATCA3DMAzDuIDcwwS9eovVkZuMse5LHjdxYagKZVRbzL4IQ3ORVlN7Tfu6rO6Km447fb/fZ7HAzUDCNCbac3rugoLeCWfIuPhEU8eGvJPO+tksvac7w7h3WtaQ3duyFvGzBvnGyxrknZY1XN/veGrWAMNBYVmDvMuWNXhvy3q+s062l8N6BgzDMAyj4uQuVRZaC4wWNf4wcGMVDbftJXW3BSTnNcrZ7RkZzn1Wobii7Wz0Y2HWlVWSzkvPLLT0/kHvhLOewx0z/jiNpHdZnGHcOy1rPW9ZW9Zh5jJrgPpSYVmD8y5Z1iDvtKx1jGWdxvVnDdPyvgjrGTAMwzCMipO7ZDl9dkB9w5U//raKRbWjgW4jGbWg2dNtFW6IhpXYRVJuXh4xOn2mGaMh76QzzKt3eJRszDsla5D3fDmDZX2OZU3Q+8ZnDdAYpWYN8p4PZ6hm1pB3uyTrGTAMwzCMipN7nQHDMAzDMOYDW2fAMAzDMIxUrBgwDMMwjIqTe0pEhtGFucDvYu1Xc/ZLUPRcu5HcC/uGOMO4d5oz3Bxvy1o0sawt65vnbVmLHi9nnQXrGTAMwzCMijM3N0vMCl8n+kUx/AITN/2DSXpX0Rmq7V1FZ6im9013Bnlb1iKvt/UMGIZhGEbFuenF01T6E63HLzl5c0aWxgl533RnkK9lLRrcXGewrJNUIetkC9XOGvJ7W8+AYRiGYVSc0vQMjMi7iOJ0/BiKr3j8+bvA0cQxfuvJFdfG+GB8BTdL76QzhL1jOkP8rEHeNz1rf/60rEHeMbP211UUlvX5+W961iCntKz9MTc9a5jtd1dpioGiP0yYvstUE9h2//b7O3Unjsm3I/TlmIUzhK/d33aS5h3DGeJnDfK2rNNfNwss6+KoYtaQ3fumZw2z/e6yYQLDMAzDqDil6RmIjd+z2ldVa64dBI69SYS8b7ozyNuyFlXwrqIzVNu7is5QnLf1DBiGYRhGxYnWM9B27ZIra/r1ly/A3yoxq4tKVk6+qvLV0KFri96bMekdcobZek9WiyHvWTrD9WcN8o6ZNcg7ZtYQ/3c8S9Ygb8u6WK7r71nZsgZ5x8wabt53l/UMGIZhGEbFmXnPgN8sot9VPXPS0DzQxlCPh3U9HvRhecZX42dZtoFT929/a4i/LcNv+uBv18hLyDvkDLP1TjpD2LsoZ5B32bIGecfMGuQdM2uQd1rWIO+YWUP833HLWhSZNci7bFmDvGNmDfKOmTXM9rvLegYMwzAMo+LMvGfg6FB1TMdVU4OelkyoLarK2hyqjlncntUdm+f4sZYO5+K+ulqc8jgvIe/rdoawd1HOIO+yZQ3yjpk1yDumM8g7LWv/2LLOR5WzBnmXLWsY94yRNcg7ZtYw2+8u6xkwDMMwjIoz056BvVGbbkODKsMdVVknyxrtWThQrfPCbcDYWVngTusNAJZndD3NROvHXSYrzKuyN5JfyDvkDMzUuznRxvAuW9Yg75hZg7xjZg3yLVvWIG/LulhiZw3yLlvWMP4/5quSJWuQd8ysfTurrPOfZ3jMyPUrjFz/xKimD6nhZl60+i0Y6gM8vnWsnz3aB6B9qikRe8tPAah/VKe2ode99Wtv5L6srPiuFH+7SK8+/nyQoRxC3mPOEPQOOQPX7n1ht1LCOzVrgGG7dFmDHGNmDfIuW9bJ54PMIGuQ9zxmDfK+6VmDvNOyBnmXLWuQd8ysQd5zk3UGbJjAMAzDMCpObTTytdF0Dg4O2NwcX9Lg0YOf0eupw2Kl+SoArWXVFs1VVU4nowbsaYLFsK0bH2quv+OFm4yx++A+AM+ftRm4aRKv31F1dfftdwBYXi16GYlzjl276lp/O4mvsmq18wkijx78TMcEvMecIegdcgaC3rN0hnHvNGcY907NGmCvZlknvMuWNcg7ZtYg73nMGuRdVNYQ3ztL1iDvtKxB3mXLGuQdM2uQd9myBnlPZg2wv7/PxsbG1PNZz4BhGIZhVJzccwaGx8ccNFRdLXeXAKivbQFQa6gq2eyvMdxSx0PdVVuDviqo4+ea/tB2syE+/OgT7n+6C0Cn9z4AX37nqwB88/fVvvWFz+a93Kn4qspv/pA27jI8Vi0W8k46A0HvkDMQ9J6lM4x7XzTWlPROyxrkbVmfe5cta8juXVTWIO95zBrkXVTWIO+yZQ3yTssa5F22rEHeMbMGec9L1lmwngHDMAzDqDi5ewa6vQHtrmqS4W037WDBT83UOAytRqLa0KKJjY5e0xmp+jo6/YXOt/Mx//ne9wF48URV24/f/w8APnrwIwC+/eff4St33817yWMMJtrmtAMTdHs6OuidcAaC3iFnIOiddAZK452WNXjvcmUN8o2ZNcjbsj73TssaivUuLGuQd0FZg7zLljU479SsAVZKl7VvY2YN8p6XrLNgPQOGYRiGUXFy9wws1fqso3Glk5oWZ1h1MyobC37xxMASDC3VH62WG+k42AHg408e8uHH/wvA7qmr0nZ/CcD9D34MwPBHv4TvfReAr7z7e3kvfQw/3uKrrMa0A5EzEPROdQZo1YPOQNB7zBlm6p3mDOPelnW2rMF5lyxryO5dVNYg79SsAb733Wt3hml/z4rJGuRdtqxB3vOYNcg7atYABztzk3UWchcDhzXonGjyxcKRPrzuqk7XdL9HC4n+C7/fs99u6uCZbtfYres1p+0eg1P/gg5J7jvdv3rvbznd1jHt77wA4Ot/+oe5rr8x0Z5MPG4FXnPo7tYIeYecYdw75AwEvZPOwEy905xh3HseswY5xswa5F22rP3jmFmD956eNci7bFmDvIvKGuRdtqxB3mlZg/MuWda+jZk1yLtsWcP0vC/ChgkMwzAMo+Lk7hlYBnhV1d5S3e33PNTpah1VR63ugMaaJlsMXKV5ePgCgMeHnwLQ3tWEi+fbHZpn+y6F2QP++Qf/pNcNdWxvVdfwjXe/roO2py+qkIavpvopx5x1HgW8k85A0DvkDKR677k25H1VZ5B3mjOMe6dlDfK2rF8A8i5b1pDdu6isIbt32bIGecfMGpx3xKxB3mlZg7wt63Pveck6C9YzYBiGYRgVJ3fPQH+5SavjljlY1lSGFztPADh2kzJaWxssnKqsGp6+AODxoVuw4ake190OUNvDVbbfVN328LGrcbp+isQ59x7rde1/+0cAjg6eA/Bff6LFH/7iz77N7Vufu7SPH29Jq676yxrzCXknnYGgd8gZyO2ddAZye19UUY55p2QN8o6ZNVzeO0bWIO+yZQ2X8C4oa5B3mjPIu2xZg7xjZg3yjpo1wPIwNWuQt2Wtx/WD7txknQXrGTAMwzCMipO7Z2B9bYPGssY++ksatWg+1RjI4ERbWi6193myqHrlaFcV2e6JHtf3tSzk6ht6/ouLX+DULQix+j+PANj5VLesdNsP9Fo4G5l58PAAgN7zfwXgeVtTPZdO9vjWH/8lAL/xpdfz6gVZX1PFGPJOOgNB75AzEPROOkPYO+kMRPFOyxrkHTNrkHfZsgZ5W9bn3mlZg7zLljXIu6isQd5pWYO8Y2YN8k7LGuQdM2so1ruorEHe85J1FqxnwDAMwzAqTv6egVe3GQ3WAWicqlLqLLmZmX1VeDuHxxyd6MbOTldV1Su3NPO0M9DWjqtuGcjX7mzym3e+DMAnX9X4zenuYwCePNZylw8f7bLzQIs89HcO9R59HfvBD38IQG9wxODpFgBPv6nZqW+/rertldemz9j0VdvCxONJZyDonXQGgt4hZyDonXQGgt5JZyDonebsPdOcJ73TsvbeMbP23mXL2nvHzPoy3jGz9t5pWXvvsmXtvYvK2nunZe29Y2btvdOy9t4xs/beZcvae5ct62neWchdDCxyCxpuysKa1m6+vaRJEwdN1+FQ63G8o+6mBf/cqda83rytyRh3Nr4EwGi7xXpTH+zXDnWe5aamRrTrmmjx6N5DPvjFRwD89KcfAvCTH7wHwM/v/R8A+x8/4h86fwfAOr/Sefb+AID6t74BwFZggSm/+3PaKk6L3HIHveyddAaC3iFnIOiddAaC3klnIOid5uy9L1q5asw7JWvvHTNr7122rL13zKy9d1rWl/IuKGvvnZa19y5b1mfeBWXtvdOy9t5Rs3beaVmfeUfM2nuXLWvvHTPrq3hnwYYJDMMwDKPi1Eaj0YW9CgcHB2xubo49l/ayQUfdKXvdPZ4/UQXWdsevrenx7XUt6LC6fFsXUqtNniaV7oGqt7//m78G4Pv/8j4Av/r0Hgs1LTV5Z0vv8dt/pKrqa7+rrpe7v/75zO+TvK6szkDQ+6rOMO6ddAaC3ld1hst7x8wa5F22rEHeMbMGeVvWF5M1a5B3zKxB3mXLGuQdM2uQd9myBnnHzBqye4eua39/n42N6cMN1jNgGIZhGBVnJj0DfkeHNksMOxorGqEJG/2Gfra54Ac/ck9bGOPn//7fAPzk/oc8fqLxl5ND3e5x980vAvDWb+l2jXfu/k7mAZbsVeW5MxD0nqUzEPQec4ZM3pf5H0TIO2bWIO/UrOHS3lfNGrx3vKxB3pb1xee9zP8WoR01a5B32bIGecfMGuRdvqyhSO8sWUP27y7rGTAMwzAM49LMqGcghF8s0e+TvXLJ12ene6T3uHdfszebKvTYXHkTgK3XNoOvC3G5qjJE0nt2zhD2vqozlDtrkHf5sobr+h23rC/Gsr6Kd7ysQd6W9eW8rWfAMAzDMIxLE7FnYBJfZanqGh3WYV1l0OXnZ2bkWItJjFZXM7/H1avKJAFnmK13wjnrexTzP4gk15t11vcpNmuQd8SsAY6Py5c1wPqSZV00lvU1Zg3Qj5o1ZP/usp4BwzAMwzAuTf6egcEu1F9xj/ysSj+2Up9ok2j1pmO0heRwT5tCPH/2hN6iVnPa3FCV9Znt1zJJ5GFwoNWlGhvey1dS4+NCY1XlwG05EfROcwboBJ2BEnhPd4ZJ7/nLGuQdM2uQd/myhuT8lRhZg7znM2uQ703PGuQ9f1mD946XNci7fFkDrOTqGch9b0Snu0C9/kIXtagPr+YWTKj1V146+8C1jYE+yPpACy+0mwqh1x3x5IF2eqp/RvdOLHT0s+3PvpH3MqdS2/CTQI5c23Str41e3o+605VQyDvkDOPeIWcg6D0LZ5jmPd0Zxr3nMWvw3vGyBnmXL2uQd7ysQd7zmDU474Kyhuv4e3Zx1iDvtKxB3uXLGuQdL2uQd/myhml5X4QNExiGYRhGxcndM3DcOWQ0UldEC03mWFhUDbWw2nMnb/BSvdHQ41FD3SqNfbcD1Poxpw1VNidPVfmM6jpf/0RLY259ftl10Fyd+tntIZO3iaii649OXnrNcUe7TYW8x531DmM06kFnIOiddAZm7D3dGca907PWO5xRkqx1VSvEzBrkXb6sAfpRswZ5p2UNRXsXkzXIu6isQd5lyxrkPZ9ZQ9a/Z0VlDfIuW9Yw/e/4xec1DMMwDKPS5J5AePL0Iay56qrrKim3HWTdj2HULlELdXt0ugcA9EaqSvsnOs+opZpla2WLWjP88uIYjLW1WuvsJydPtRVlyDuvMxD0TjoDM/ae7gzj3pb1vGcN8o2XNcg7LWu4vt/xtKzBeReUNci7bFmD87as5zxrtZNZg91aaBiGYRjGBeSeM9Crt1gduZmZ677kcbMYh6pQRrXF7IswNBdpNW8B52MrdVfcdNzp+/0+iwVuBhKmMdGe03MXFPROOEPGxSeaOjbknXTWz2bpPd0Zxr3Tsobs3pa1iJ81yDde1iDvtKzh+n7HU7MGGA4KyxrkXbaswXtb1vOddbK9HNYzYBiGYRgVJ3epstBaYLSo8YeBG6touG0vqbstIDmvUc7u1cxw7rMKxRVtZ6MfC7OurJJ0XnpmoaX3D3onnPUc7pjxx2kkvcviDOPeaVnrecvasg4zl1kD1JcKyxqcd8myBnmnZa1jLOs0rj9rmJb3ReS+ytNnB9Q3nLG/rWJR7Wig20hGLWj2dFuF65VhJfbnkpuXO4lOn2mSSMg76Qzz6h3uGBvzTska5D1fzmBZn2NZE/S+8VkDNEapWYO858MZqpk15N0hwYYJDMMwDKPi5L610DAMwzCM+cBuLTQMwzAMIxUrBgzDMAyj4lgxYBiGYRgVJ/f8yAxTDeYCv4u139rBL0HRc+1Gci/sG+IM495pznBzvC1r0cSytqxvnrdlLXq8nHUWrGfAMAzDMCrO3Nw5OSt8negXxfALTNz0DybpXUVnqLZ3FZ2hmt433RnkbVmLvN43/fOaSn+i9fhVpm5OZ9I4Ie+b7gzytaxFg5vrDJZ1kipknWyh2llDfm8bJjAMwzCMilOanoEReRdRnI7vNvEVjz9/FziaOMbvNrXi2hgfjK/gZumddIawd0xniJ81yPumZ+3Pn5Y1yDtm1v66isKyPj//Tc8a5JSWtT/mpmcNs/3usp4BwzAMw6g4pekZKLqygum7TDWBbfdvv79Td+KYfDtCX45ZOEP42v1tJ2neMZwhftYgb8s6/XWzwLIujipmDdm9b3rWMNvvLusZMAzDMIyKU5qegdj4Pat9VbXm2kHg2JtEyPumO4O8LWtRBe8qOkO1vavoDMV5W8+AYRiGYVScaD0DbdcuubKmX3/5Avx9k7O6qGTl5KsqXw0durbojZqT3iFnmK33ZLUY8p6lM1x/1iDvmFmDvGNmDfF/x7NkDfK2rIvluv6elS1rkHfMrOHmfXfNvBjw60P3u1I4aWjqR2Oox8O6Hg/6sDzjq/ETK9rAqfu3vzXE35bh13n2t2vkJeQdcobZeiedIexdlDPIu2xZg7xjZg3yjpk1yDsta5B3zKwh/u+4ZS2KzBrkXbasQd4xswZ5x8waZvvdZcMEhmEYhlFxZt4zcHSoOqbjqqlBT0sm1BZVZW0OVccsbs/qJo1zfPdKh3NxX10tTnmcl5D3dTtD2LsoZ5B32bIGecfMGuQd0xnknZa1f2xZ56PKWYO8y5Y1jHvGyBrkHTNrmO13l/UMGIZhGEbFmWnPwN6oTbehQZXhjqqsk2WN9iwcqNZ54fZc6qwscKf1BgDLM7qeZqL14y6TFeZV2RvJL+QdcgZm6t2caGN4ly1rkHfMrEHeMbMG+ZYta5C3ZV0ssbMGeZctaxj/H/NVyZI1yDtm1r6dVdbWM2AYhmEYFSd/UTE8ZuRKiZEbrBjVVDE13DTMVr8FQ1VTx7eO9bNH+wC0TzU/cm/5KQD1j+rUNvS6t37tjdyXlRU/ruJvF+nVx58PMpRDyHvMGYLeIWfg2r0vHGNKeKdmDTBsly5rkGPMrEHeZcs6+XyQGWQN8p7HrEHeNz1rkHda1iDvsmUN8o6ZNch7brLOgPUMGIZhGEbFqY1GvjaazsHBAZub40saPHrwM3o9jV6sNF8FoLWs2qK5qsrpZNSAPc22HLZ1F2TNDX68cDMzdx/cB+D5szYDN2fy9Tuqru6+/Q4Ay6tFLyNxzrFrV13r7y31VVatdj5b9NGDn+mYgPeYMwS9Q85A0HuWzjDuneYM496pWQPs1SzrhHfZsgZ5x8wa5D2PWYO8i8oa4ntnyRrknZY1yLtsWYO8Y2YN8i5b1iDvyawB9vf32djYmHo+6xkwDMMwjIqTe87A8PiYg4aqq+XuEgD1tS0Aag1VJZv9NYZb6niou2pr0FcFdfxccyHbbmrkhx99wv1PdwHo9N4H4MvvfBWAb/6+2re+8Nm8lzsVX1X5zR/Sxl2Gx6rFQt5JZyDoHXIGgt6zdIZx74vGmpLeaVmDvC3rc++yZQ3ZvYvKGuQ9j1mDvIvKGuRdtqxB3mlZg7zLljXIO2bWIO95yToLuYuBbm9Au6vLGN52Iw0LfjaGul5oNRJdD1o0sdHRazojfeBHp7/Q+XY+5j/f+z4AL54oqB+//x8AfPTgRwB8+8+/w1fuvpv3kscYTLTNaQcm6PZ0dNA74QwEvUPOQNA76QyUxjsta/De5coa5Bsza5C3ZX3unZY1FOtdWNYg74KyBnmXLWtw3qlZA6yULmvfxswa5D0vWWfBhgkMwzAMo+Lk7hlYqvVZR11JJzUtzrDqJlE0FvziiYElGFqqP1ot17lxsAPAx5885MOP/xeA3VNXpe3+EoD7H/wYgOGPfgnf+y4AX3n39/Je+hi+i8VXWY1pByJnIOid6gzQqgedgaD3mDPM1DvNGca9LetsWYPzLlnWkN27qKxB3qlZA3zvu9fuDNP+nhWTNci7bFmDvOcxa5B31KwBDnbmJussWM+AYRiGYVSc3D0DhzXonGjyxcKRKqnuqk7XdEXlQmIww+/37PeePHim2zV263rNabvH4NS/oEOS+672+av3/pbTbR3T/s4LAL7+p3+Y6/obE+3JxONW4DWH7m6NkHfIGca9Q85A0DvpDMzUO80Zxr3nMWuQY8ysQd5ly9o/jpk1eO/pWYO8y5Y1yLuorEHeZcsa5J2WNTjvkmXt25hZg7zLljVMz/sirGfAMAzDMCpO7p6BZYBXVe0t1VVGngx1ulpH1VGrO6CxppmXA1dpHh6+AODx4acAtHc1+/L5dofm2SaMYfaAf/7BP+l1Qx3bW9U1fOPdr+ug7emLKqThq6l+yjFnI0kB76QzEPQOOQOp3nuuDXlf1RnkneYM495pWYO8LesXgLzLljVk9y4qa8juXbasQd4xswbnHTFrkHda1iBvy/rce16yzoL1DBiGYRhGxcndM9BfbtLquGUOljWv8cXOEwCO3QzN1tYGC6cqq4anLwB4fOgWbHiqx3W3HeT2cJXtN1W3PXzsapyuny95zr3Hel373/4RgKOD5wD8159o8Ye/+LNvc/vW5y7t48db0qqr/rLGfELeSWcg6B1yBnJ7J52B3N4XVZRj3ilZg7xjZg2X946RNci7bFnDJbwLyhrkneYM8i5b1iDvmFmDvKNmDbA8TM0a5G1Z63H9oDs3WWchdzGwvrZBY1ndHf0ldVQ0n6rbY3CiXayW2vs8WdQlHu0qhN0TPa7vayWo1Tf0/BcXv8CpWxBi9X8eAbDzqW5Z6bYf6LVw1hnz4OEBAL3n/wrA87Zmdyyd7PGtP/5LAH7jS6/n1QuyvqZfkpB30hkIeoecgaB30hnC3klnIIp3WtYg75hZg7zLljXI27I+907LGuRdtqxB3kVlDfJOyxrkHTNrkHda1iDvmFlDsd5FZQ3ynpess2DDBIZhGIZRcfL3DLy6zWiwDkDjVJVSZ8lNxuirwts5POboRPdydLqqql65pckmnYF2c1p1y0C+dmeT37zzZQA++aq6bE53HwPw5LGWu3z4aJedB1rkob9zqPfo69gPfvhDAHqDIwZPtwB4+k1NSHn7bVVvr7w2fZKGr9oWJh5POgNB76QzEPQOOQNB76QzEPROOgNB7zRn75nmPOmdlrX3jpm19y5b1t47ZtaX8Y6ZtfdOy9p7ly1r711U1t47LWvvHTNr752WtfeOmbX3LlvW3rtsWU/zzoL1DBiGYRhGxcndM7DILWi4KQtr2sjh9pImTRw0XY1R63G8o7GnBf/cqTbA2LytyRh3Nr4EwGi7xXpTVdbXDnWe5aamRrTrmmjx6N5DPvjFRwD89KcfAvCTH7wHwM/v/R8A+x8/4h86fwfAOr/Sefb+AID6t74BwFZgtUm/+3Pako6L3HIHveyddAaC3iFnIOiddAaC3klnIOid5uy9L1rGcsw7JWvvHTNr7122rL13zKy9d1rWl/IuKGvvnZa19y5b1mfeBWXtvdOy9t5Rs3beaVmfeUfM2nuXLWvvHTPrq3hnwXoGDMMwDKPi1Eaj0YVDDAcHB2xubo49l/ayQUdjK3vdPZ4/UQXWdsevrenx7XUt6LC6fFsXUqtNniaV7oGqt7//m78G4Pv/8j4Av/r0Hgs1LTV5Z0vv8dt/pKrqa7+rcZi7v/75zO+TvK6szkDQ+6rOMO6ddAaC3ld1hst7x8wa5F22rEHeMbMGeVvWF5M1a5B3zKxB3mXLGuQdM2uQd9myBnnHzBqye4eua39/n42N6XMPrGfAMAzDMCrOTHoG/I4ObZYYdjRWNEKzN/sN/WxzwQ9+5J62MMbP//2/AfjJ/Q95/ETjLyeHuvfz7ptfBOCt39K9m+/c/Z3MAyzZq8pzZyDoPUtnIOg95gyZvC/zP4iQd8ysQd6pWcOlva+aNXjveFmDvC3ri897mf8tQjtq1iDvsmUN8o6ZNci7fFlDkd5Zsobs3115egZmVAyE8Osj+X2yVy75+ux0j/Qe9+5rwkZT2bK58iYAW69tBl8X4nK/SCGS3rNzhrD3VZ2h3FmDvMuXNVzX77hlfTGW9VW842UN8rasL+dtwwSGYRiGYVyaiD0Dk/gqS1XX6LAO6yqDLj8lIyPHWkxitLqa+T2uXlUmCTjDbL0Tzlnfo5j/QSS53qyzvk+xWYO8I2YNcHxcvqwB1pcs66KxrK8xa4B+1Kwh+3eX9QwYhmEYhnFp8vcMDHah/op75CdS+LGV+kSbRAs2HKNdo4Z72hTi+bMn9Ba1gMPmhqqsz2y/lkkiD4MDLSjR2PBevpIaHxcaqyoHbsuJoHeaM0An6AyUwHu6M0x6z1/WIO+YWYO8y5c1JOevxMga5D2fWYN8b3rWIO/5yxq8d7ysQd7lyxpgxXoGDMMwDMO4PLnvjeh0F6jXXwDQWFRNUXMLJtT6Ky+dfeDaxkBVVX2ghRfaTVVkve6IJw+07WP9M7p3YqGjn21/9o28lzmV2oafEXrk2qZrfUfJy/tRd7oSCnmHnGHcO+QMBL1n4QzTvKc7w7j3PGYN3jte1iDv8mUN8o6XNch7HrMG511Q1nAdf88uzhrknZY1yLt8WYO842UN8i5f1jAt74uwngHDMAzDqDi5ewaOO4eMRhqXaKGZnQuLqqEWVnvu5A1eqjcaejxqaIylse+2g1w/5rShyubkqSqfUV3n659oacytzy+70ZqrUz+7V3TynlFVdP3RyUuvOe5o68mQ97iz3mGMRj3oDAS9k87AjL2nO8O4d3rWeoczSpK1rmqFmFmDvMuXNUA/atYg77SsoWjvYrIGeReVNci7bFmDvOcza8j696yorEHeZcsapv8dv/i8hmEYhmFUmtx3E5w8fQhrrrrqukrKbQdZ92MYtUvUQt0ene4BAL2RqtL+ic4zaqlm2VrZotYMv7w4BmNtrdY6+8nJU21FGfLO6wwEvZPOwIy9pzvDuLdlPe9Zg3zjZQ3yTssaru93PC1rcN4FZQ3yLlvW4Lwt6znPWu1k1nDx3QS5hwl69RarIzcZY91buokLQ13UqLaYfRGG5iKtpvaa9lHUnU/Hnb7f77NY4PrfYRoT7Tk9d0FB74QzZFx8oqljQ95JZ/1slt7TnWHcOy1ryO5tWYv4WYN842UN8k7LGq7vdzw1a4DhoLCsQd5lyxq8t2U931kn28thwwSGYRiGUXFylyoLrQVGi+pyGLjuiYbb6Yq62/WJ8xrl7PaMDOc+q1Bc0XbW4bEw68oqSeelZxZaev+gd8JZz+GOGX+cRtK7LM4w7p2WtZ63rC3rMHOZNUB9qbCswXmXLGuQd1rWOsayTuP6s4ZpeV+E9QwYhmEYRsXJXbKcPjugvuHKH39bxaLa0UC3kYxa0Ozptgo3RMNK7CIpNy+PGJ0+0ySRkHfSGebVOzxKNuadkjXIe76cwbI+x7Im6H3jswZojFKzBnnPhzNUM2vIu12S9QwYhmEYRsXJfWuhYRiGYRjzgW1UZBiGYRhGKlYMGIZhGEbFyT0lIsPowlzgd7H2qzn7JSh6rt1I7oV9Q5xh3DvNGW6Ot2UtmljWlvXN87asRY+Xs86C9QwYhmEYRsWZm5slZoWvE/2iGH6BiZv+wSS9q+gM1fauojNU0/umO4O8LWuR19t6BgzDMAyj4tz04mkq/YnW45ecvDkjS+OEvG+6M8jXshYNbq4zWNZJqpB1soVqZw35va1nwDAMwzAqTml6BkbkXURxOn4MxVc8/vxd4GjiGL/15IprY3wwvoKbpXfSGcLeMZ0hftYg75uetT9/WtYg75hZ++sqCsv6/Pw3PWuQU1rW/pibnjXM9rvLegYMwzAMo+KUpmeg6MoKpm852QS23b/9Zo/diWOybFd5VWbhDOFr9/egpnnHcIb4WYO8Lev0180Cy7o4qpg1ZPe+6VnDbL+7SlMMxMbvWe0/yDXXDgLH3iRC3jfdGeRtWYsqeFfRGartXUVnKM7bhgkMwzAMo+JE6xlou3bJlTX9+ssX4G+VmNVFJSsnX1X5aujQtUXvzZj0DjnDbL0nq8WQ9yyd4fqzBnnHzBrkHTNriP87niVrkLdlXSzX9fesbFmDvGNmDTfvu8t6BgzDMAyj4sy8Z8BvFtHvqp45aWjqR2Oox8O6Hg/6sDzjq/ETK9rAqfu3vzXE35bhN33wt2vkJeQdcobZeiedIexdlDPIu2xZg7xjZg3yjpk1yDsta5B3zKwh/u+4ZS2KzBrkXbasQd4xswZ5x8waZvvdZT0DhmEYhlFxZt4zcHSoOqbjqqlBT0sm1BZVZW0OVccsbs/qJo1z/FhLh3NxX10tTnmcl5D3dTtD2LsoZ5B32bIGecfMGuQd0xnknZa1f2xZ56PKWYO8y5Y1jHvGyBrkHTNrmO13l/UMGIZhGEbFmWnPwN6oTbehQZXhjqqsk2WN9iwcqNZ54TZg7KwscKf1BgDLM7qeZqL14y6TFeZV2RvJL+QdcgZm6t2caGN4ly1rkHfMrEHeMbMG+ZYta5C3ZV0ssbMGeZctaxj/H/NVyZI1yDtm1r6dVdbWM2AYhmEYFSd/UTE8ZuRKiZEbrBjVVDE13DTMVr8FQ1VTx7eO9bNH+wC0TzU/cm/5KQD1j+rUNvS6t37tjdyXlRU/ruLvHe3Vx58PMpRDyHvMGYLeIWfg2r0vHGNKeKdmDTBsly5rkGPMrEHeZcs6+XyQGWQN8p7HrEHeNz1rkHda1iDvsmUN8o6ZNch7brLOQG008h/HdA4ODtjcHF/S4NGDn9HrqcNipfkqAK1lXVVzVR/WyagBe5pgMWzrxoea6+944SZj7D64D8DzZ20GbprE63f0gd59+x0AlleLXkbinGPXrrrW307iP9ha7XyCyKMHP9MxAe8xZwh6h5yBoPcsnWHcO80Zxr1TswbYq1nWCe+yZQ3yjpk1yHseswZ5F5U1xPfOkjXIOy1rkHfZsgZ5x8wa5F22rEHek1kD7O/vs7GxMfV8NkxgGIZhGBUn9zDB8PiYg4aqq+XuEgD1tS0Aag1VJZv9NYZb6niou2pr0FcFdfxc0x/abjbEhx99wv1PdwHo9N4H4MvvfBWAb/6+2re+8Nm8lzsVX1X5zR/SulqGx6rFQt5JZyDoHXIGgt6zdIZx74u6l5LeaVmDvC3rc++yZQ3ZvYvKGuQ9j1mDvIvKGuRdtqxB3mlZg7zLljXIO2bWIO95yToL1jNgGIZhGBUnd89Atzeg3VVNMrztph0s+NkYGoeh1UhUG1o0sdHRazojVV9Hp7/Q+XY+5j/f+z4AL56oavvx+/8BwEcPfgTAt//8O3zl7rt5L3mMwUTbnHZggm5PRwe9E85A0DvkDAS9k85AabzTsgbvXa6sQb4xswZ5W9bn3mlZQ7HehWUN8i4oa5B32bIG552aNcBK6bL2bcysQd7zknUWrGfAMAzDMCpO7p6BpVqfdTSudFLT4gyrbkZlY8EvnhhYgqGl+qPVciMdBzsAfPzJQz78+H8B2D11VdruLwG4/8GPARj+6Jfwve8C8JV3fy/vpY/hx1t8ldWYdiByBoLeqc4ArXrQGQh6jznDTL3TnGHc27LOljU475JlDdm9i8oa5J2aNcD3vnvtzjDt71kxWYO8y5Y1yHseswZ5R80a4GBnbrLOgvUMGIZhGEbFyd0zcFiDzolmYi4cqZLqrup0TVdULiQGM/r+H27vyYNnundzt67XnLZ7DE79Czokue9qn79672853dYx7e+8AODrf/qHua6/MdGeTDxuBV5z6G7dDHmHnGHcO+QMBL2TzsBMvdOcYdx7HrMGOcbMGuRdtqz945hZg/eenjXIu2xZg7yLyhrkXbasQd5pWYPzLlnWvo2ZNci7bFnD9LwvIncxsAzwqgJeqrv9noc6Xa2jD6TVHdBY02SLgfvlOjx8AcDjw08BaO9qwsXz7Q7Ns32XwuwB//yDf9Lrhjq2t6pr+Ma7X9dB29MXVUjDf4D9lGPOOo8C3klnIOgdcgZSvfdcG/K+qjPIO80Zxr3TsgZ5W9YvAHmXLWvI7l1U1pDdu2xZg7xjZg3OO2LWIO+0rEHelvW597xknQUbJjAMwzCMipO7Z6C/3KTVccscLGsqw4udJwAcu0kZra0NFk5VVg1PXwDw+NAt2PBUj+tuB6jt4Srbb6pue/jY1ThdP0XinHuP9br2v/0jAEcHzwH4rz/R4g9/8Wff5vatz13ax3expFVX/WV184S8k85A0DvkDOT2TjoDub0vqijHvFOyBnnHzBou7x0ja5B32bKGS3gXlDXIO80Z5F22rEHeMbMGeUfNGmB5mJo1yNuy1uP6QXduss6C9QwYhmEYRsXJ3TOwvrZBY1ljH/0ljVo0n2oMZHCiXayW2vs8WVS9crSrimz3RI/r+1oWcvUNPf/FxS9w6haEWP2fRwDsfKpbVrrtB3otnI3MPHh4AEDv+b8C8Lyt2R1LJ3t864//EoDf+NLrefWCrK+pYgx5J52BoHfIGQh6J50h7J10BqJ4p2UN8o6ZNci7bFmDvC3rc++0rEHeZcsa5F1U1iDvtKxB3jGzBnmnZQ3yjpk1FOtdVNYg73nJOgvWM2AYhmEYFSd/z8Cr24wG6wA0TlUpdZbczMy+Krydw2OOTnQvR6erquqVW5p52hloa8dVtwzka3c2+c07Xwbgk69q/OZ09zEATx5rucuHj3bZeaBFHvo7h3qPvo794Ic/BKA3OGLwdAuAp9/U7NS331b19spr02ds+qptYeLxpDMQ9E46A0HvkDMQ9E46A0HvpDMQ9E5z9p5pzpPeaVl775hZe++yZe29Y2Z9Ge+YWXvvtKy9d9my9t5FZe2907L23jGz9t5pWXvvmFl777Jl7b3LlvU07yxYz4BhGIZhVJzcPQOL3IKGm7+4po0cbi9pBuVB09UYtR7HOxp7WvDPnWoDjM3bmpl5Z+NLAIy2W6w3VWV97VDnWW5qnmS7rlmXj+495INffATAT3/6IQA/+cF7APz83v8BsP/xI/6h83cArPMrnWfvDwCof+sbAGwFVpt0a1GkLum4yC130MveSWcg6B1yBoLeSWcg6J10BoLeac7e+6JlLMe8U7L23jGz9t5ly9p7x8zae6dlfSnvgrL23mlZe++yZX3mXVDW3jsta+8dNWvnnZb1mXfErL132bL23jGzvop3Fmqj0ejCXoWDgwM2NzfHnkt72aCj7pS97h7Pn+hDb7vj19b0+Pa6FnRYXb6tC6nVJk+TSvdAgf393/w1AN//l/cB+NWn91ioaXWpO1t6j9/+I32QX/tddb3c/fXPZ36f5HVldQaC3ld1hnHvpDMQ9L6qM1zeO2bWIO+yZQ3yjpk1yNuyvpisWYO8Y2YN8i5b1iDvmFmDvMuWNcg7ZtaQ3Tt0Xfv7+2xsTB9usGECwzAMw6g4M+kZ8Is4t1li2FH30AhN2Og39LPNBd/fkXukYoyf//t/A/CT+x/y+Im6XE4OdbvH3Te/CMBbv6XbNd65+zuZ+1SyV5XnzkDQe5bOQNB7zBkyeV/mfxAh75hZg7xTs4ZLe181a/De8bIGeVvWF5/3Mv9bhHbUrEHeZcsa5B0za5B3+bKGIr2zZA3Zv7usZ8AwDMMwjEszo56BEH6xRL9P9solX5+d7pHe4959TdhoqtBjc+VNALZe2wy+LsTlqsoQSe/ZOUPY+6rOUO6sQd7lyxqu63fcsr4Yy/oq3vGyBnlb1pfztp4BwzAMwzAuTcSegUl8laWqa3RYh3WVQZefn5mRYy0mMVpdzfweV68qkwScYbbeCees71HM/yCSXG/WWd+n2KxB3hGzBjg+Ll/WAOtLlnXRWNbXmDVAP2rWkP27y3oGDMMwDMO4NPl7Bga7UH/FPfKzKv3YSn2iTaIFG47RFpLDPW0K8fzZE3qLWsBhc0NV1me2X8skkYfBgRaUaGx4L19JjY8LjVWVA7flRNA7zRmgE3QGSuA93Rkmvecva5B3zKxB3uXLGpLzV2JkDfKez6xBvjc9a5D3/GUN3jte1iDv8mUNsGI9A4ZhGIZhXJ7cN0p2ugvU6y8AaCyqpqi51ZNq/ZWXzj5wbWOgqqo+0CpM7aYqsl53xJMH2vax/hndSLnQ0c+2P/tG3sucSm3Dzwg9cm3Ttb6jZPjSazpdCYW8Q84w7h1yBoLes3CGad7TnWHcex6zBu8dL2uQd/myBnnHyxrkPY9Zg/MuKGu4jr9nF2cN8k7LGuRdvqxB3vGyBnmXL2uYlvdF5C4GjjuHjEbqimihyRwLi/rYFlZ77uQNXup8aOjxqKFulca+2wFq/ZjThmROnkp2VNf5+idaGnPr88uug+bq1M9uD5m8TUQh9kcnL73muKPdpkLe4856hzEa9aAzEPROOgMz9p7uDOPe6VnrHc4oSda6qhViZg3yLl/WAP2oWYO807KGor2LyRrkXVTWIO+yZQ3yns+sIevfs6KyBnmXLWuY/nf84vMahmEYhlFpck8gPHn6ENZcddV1lZTbAaruuy1ql6iFuj063QMAeiNVpf0TnWfUUs2ytbJFrRl+eXEMxtparXX2k5On2n0q5J3XGQh6J52BGXtPd4Zxb8t63rMG+cbLGuSdljVc3+94WtbgvAvKGuRdtqzBeVvWc5612smswSYQGoZhGIZxAbnnDPTqLVZHbjLGui953MSFoSqUUW0x+yIMzUVaTe017euyuituOu70/X6fxQI3AwnTmGjP6bkLCnonnCHj4hNNHRvyTjrrZ7P0nu4M495pWUN2b8taxM8a5Bsva5B3WtZwfb/jqVkDDAeFZQ3yLlvW4L0t6/nOOtleDusZMAzDMIyKk7tUWWgtMFrU+MPAjVU03LaX1N0WkJzXKGe3Z2Q491mF4oq2s9GPhVlXVkk6Lz2z0NL7B70TznoOd8z44zSS3mVxhnHvtKz1vGVtWYeZy6wB6kuFZQ3Ou2RZg7zTstYxlnUa1581TMv7IqxnwDAMwzAqTu6S5fTZAfUNV/74eywX1Y4Guqd01IJmT/dYuiEaVmIXSbl5ecTo9JlmjIa8k84wr97hUbIx75SsQd7z5QyW9TmWNUHvG581QGOUmjXIez6coZpZQ97tknLfWmgYhmEYxnxgtxYahmEYhpGKFQOGYRiGUXEyFQMZRhIMwzAMwygpF32PZyoGDg8PC7kYwzAMwzDic9H3eKYJhMPhkIcPH7K+vk6tlm+momEYhmEYcRmNRhweHvL6669Tr0///3+mYsAwDMMwjJuLTSA0DMMwjIpjxYBhGIZhVBwrBgzDMAyj4lgxYBiGYRgVx4oBwzAMw6g4VgwYhmEYRsWxYsAwDMMwKs7/A7BFGfBfiyjWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(e_indices):\n",
    "    min_encodings = torch.zeros(e_indices.shape[0], params['n_embeddings']).to(device)\n",
    "    min_encodings.scatter_(1, e_indices, 1)\n",
    "    e_weights = model.vector_quantization.embedding.weight\n",
    "    z_q = torch.matmul(min_encodings, e_weights).view((params[\"batch_size\"],8,8,params[\"embedding_dim\"])) \n",
    "    z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    " \n",
    "    x_recon = model.decoder(z_q)\n",
    "    return x_recon, z_q,e_indices\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def uniform_samples(model):\n",
    "    rand = np.random.randint(params['n_embeddings'], size=(2048, 1))\n",
    "    min_encoding_indices = torch.tensor(rand).long().to(device)\n",
    "\n",
    "    x_recon, z_q, e_indices = generate_samples(min_encoding_indices)\n",
    "\n",
    "    # 코드워드 사용 횟수 카운트\n",
    "    counts = torch.bincount(min_encoding_indices.flatten(), minlength=params['n_embeddings']).cpu().numpy()\n",
    "\n",
    "    # 평균과 분산 출력\n",
    "    usage_mean = np.mean(counts)\n",
    "    usage_var = np.var(counts)\n",
    "    print(f\"📊 Codeword usage mean: {usage_mean:.2f}\")\n",
    "    print(f\"📉 Codeword usage variance: {usage_var:.2f}\")\n",
    "\n",
    "    # ✅ [사용 횟수 → 코드워드 개수]로 다시 집계\n",
    "    # 예: 3번 사용된 코드워드가 20개면 result[3] = 20\n",
    "    usage_histogram = np.bincount(counts)\n",
    "\n",
    "    # x축: 사용 횟수 (0 ~ max), y축: 해당 사용 횟수를 가진 코드워드 수\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(np.arange(len(usage_histogram)), usage_histogram)\n",
    "    plt.xlabel(\"Usage Count (per codeword)\")\n",
    "    plt.ylabel(\"Number of Codewords\")\n",
    "    plt.title(\"Codeword Usage Histogram (Count → #Codewords)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return x_recon, z_q, e_indices\n",
    "\n",
    "\n",
    "\n",
    "x_val_recon, z_q, e_indices = uniform_samples(model)\n",
    "display_image_grid(x_val_recon)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct from PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_folder_path = os.getcwd() \n",
    "data_file_path = data_folder_path + '/data/latent_samples.npy'\n",
    "\n",
    "samples = np.load(data_file_path,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reconstruct_from_pixelcnn(model,samples):\n",
    "    \n",
    "\n",
    "    min_encoding_indices = torch.tensor(samples).reshape(-1,1).long().to(device)\n",
    "    x_recon, z_q,e_indices = generate_samples(min_encoding_indices)\n",
    "    \n",
    "    return x_recon, z_q,e_indices\n",
    "\n",
    "\n",
    "x_val_recon,z_q,e_indices = reconstruct_from_pixelcnn(model,samples)\n",
    "\n",
    "display_image_grid(x_val_recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. load all data\\n2. compute COM for all data\\n3. get representation indices for all data and hash them\\n4. organize all data by index for steps 1-3\\n5. build unique color scheme for all used representations\\n6. iterate through hash values and color the COM pixel with hash values\\n\\ndisplay resulting image\\n\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. load all data\n",
    "2. compute COM for all data\n",
    "3. get representation indices for all data and hash them\n",
    "4. organize all data by index for steps 1-3\n",
    "5. build unique color scheme for all used representations\n",
    "6. iterate through hash values and color the COM pixel with hash values\n",
    "\n",
    "display resulting image\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "def encode_data(data,model):\n",
    "    x = data.data # assumes youre using Pytorch formatted dataset\n",
    "    x = torch.tensor(x).float().to(device)\n",
    "    x = x.permute(0,3,1,2).contiguous()\n",
    "    x = x.to(device)\n",
    "    vq_encoder_output = model.pre_quantization_conv(model.encoder(x))\n",
    "    _, z_q, _, _,e_indices = model.vector_quantization(vq_encoder_output)\n",
    "    \n",
    "    x_recon = model.decoder(z_q)\n",
    "    return x,x_recon, z_q,e_indices\n",
    "\n",
    "def count_and_hash_representations(e_indices,prune=2):\n",
    "    x_hashes = []\n",
    "    d = {}\n",
    "    n = int(len(e_indices)/64)\n",
    "    for i in range(n):\n",
    "        k = e_indices[64*i:64*i+64].squeeze().cpu().detach().numpy()\n",
    "        hash_ = hash(tuple(k))\n",
    "\n",
    "        if hash_ not in d:\n",
    "            d[hash_] = 1\n",
    "        else:\n",
    "            d[hash_]+=1\n",
    "            \n",
    "        x_hashes.append(hash_)\n",
    "            \n",
    "    # prune hash table\n",
    "    d = dict((k, v) for k, v in d.items() if v >= prune)\n",
    "    \n",
    "    return d,x_hashes\n",
    "\n",
    "\n",
    "def create_color_template(n):\n",
    "    num_shades = n\n",
    "    if n < 1000:\n",
    "        sns.palplot(sns.husl_palette(num_shades))\n",
    "        color_list = sns.husl_palette(num_shades)\n",
    "    else:\n",
    "        sns.palplot(sns.cubehelix_palette(num_shades))\n",
    "        color_list = sns.cubehelix_palette(num_shades)\n",
    "        \n",
    "\n",
    "    rgb_list = []\n",
    "    for color in color_list:\n",
    "        rgb = []\n",
    "        for value in color:\n",
    "            value *= 255\n",
    "            rgb.append(int(value))\n",
    "        rgb_list.append(np.array(rgb).astype(int))\n",
    "\n",
    "    return rgb_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 28728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "PRUNE = 0\n",
    "N_TOP_REPS = 500\n",
    "\n",
    "# get data and discrete indices\n",
    "data,_,_,e_indices = encode_data(training_data,model)\n",
    "\n",
    "# create a hash table with most common representations\n",
    "d, data_hashes = count_and_hash_representations(e_indices,PRUNE)\n",
    "\n",
    "d = dict(Counter(d).most_common(N_TOP_REPS))\n",
    "\n",
    "# count num of reps\n",
    "n = len(d.keys())\n",
    "print(n)\n",
    "colors = create_color_template(n)\n",
    "color_hash_table = {k:v for k,v in zip(d.keys(),colors)}\n",
    "\n",
    "data = data.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "color_img = np.zeros((32,32,3))\n",
    "\n",
    "colored_data = data.copy()\n",
    "\n",
    "colored_idxs = []\n",
    "\n",
    "colored_img_dict = {}\n",
    "\n",
    "for k,(x,rep) in enumerate(zip(data,data_hashes)):\n",
    "    x = np.transpose(x,(1,2,0))\n",
    "    \n",
    "    block_ij = np.argwhere(x[:,:,1]>100)\n",
    "    \n",
    "    x_min = np.min(block_ij,0)\n",
    "    x_max = np.max(block_ij,0)\n",
    "    \n",
    "    i,j = (x_min + x_max)//2\n",
    "    \n",
    "    if rep not in color_hash_table:\n",
    "        color = np.array([255,255,255])\n",
    "        for idx in block_ij:\n",
    "            row,col = idx\n",
    "            colored_data[k,0,row,col] = color[0]\n",
    "            colored_data[k,1,row,col] = color[1]\n",
    "            colored_data[k,2,row,col] = color[2]\n",
    "    else:\n",
    "        colored_idxs.append(k)\n",
    "        \n",
    "            \n",
    "        color = np.array(color_hash_table[rep])\n",
    "        color_img[i,j] = color\n",
    "        for idx in block_ij:\n",
    "            row,col = idx\n",
    "            colored_data[k,0,row,col] = color[0]\n",
    "            colored_data[k,1,row,col] = color[1]\n",
    "            colored_data[k,2,row,col] = color[2]\n",
    "            color_img[row,col]=color\n",
    "            \n",
    "        if rep not in colored_img_dict:\n",
    "            colored_img_dict[rep] = [colored_data[k,:,:,:]]\n",
    "        else:\n",
    "            colored_img_dict[rep].append(colored_data[k,:,:,:])\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.transpose(data[8],(1,2,0))\n",
    "x.shape\n",
    "\n",
    "imgplot = plt.imshow(color_img.astype(int))\n",
    "#imgplot.axes.get_xaxis().set_visible(False)\n",
    "#imgplot.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image_list = []\n",
    "\n",
    "for rep_imgs in list(colored_img_dict.values()):\n",
    "    image_list.append(torch.tensor(np.array(rep_imgs)[:4]).int())\n",
    "\n",
    "x = torch.cat(image_list[:10])\n",
    "for _ in range(2):\n",
    "    display_image_grid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow can we build a graph from a color image?\\n\\nStep 1: set up\\n\\n- Create an index to representation map (e.g. an array of all the representations) of length N\\n- Create a color to index hash table\\n- Create new image representation that takes color image and replaces all RGB colors with indices\\n- Create a N X N matrix of zeros\\n\\nStep 2: graph construction\\n\\nScan the color image twice - horizontally and vertically\\n\\n1. start at origin pixel (0,0), mark its color\\nWhile pixels not exhausted\\n2. move one step\\n3. check if color at this step is the same as previous color\\n4. if color is same, skip step 5\\n5. if color is different, add 1 to matrix[i][j] and matrix[j][i] where i is previous color and j is current color\\n6. mark current pixel color and move on to next pixel\\n\\ndo this twice for vertical and horizontal pixels\\n'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "How can we build a graph from a color image?\n",
    "\n",
    "Step 1: set up\n",
    "\n",
    "- Create an index to representation map (e.g. an array of all the representations) of length N\n",
    "- Create a color to index hash table\n",
    "- Create new image representation that takes color image and replaces all RGB colors with indices\n",
    "- Create a N X N matrix of zeros\n",
    "\n",
    "Step 2: graph construction\n",
    "\n",
    "Scan the color image twice - horizontally and vertically\n",
    "\n",
    "1. start at origin pixel (0,0), mark its color\n",
    "While pixels not exhausted\n",
    "2. move one step\n",
    "3. check if color at this step is the same as previous color\n",
    "4. if color is same, skip step 5\n",
    "5. if color is different, add 1 to matrix[i][j] and matrix[j][i] where i is previous color and j is current color\n",
    "6. mark current pixel color and move on to next pixel\n",
    "\n",
    "do this twice for vertical and horizontal pixels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Create an index to representation map\n",
    "# add black (None representation)\n",
    "index_rep_map = [np.array([0,0,0])]\n",
    "# add representations\n",
    "index_rep_map+=list(color_hash_table.values())\n",
    "index_rep_map = np.array(index_rep_map)\n",
    "\n",
    "# Create a color to index hash table\n",
    "color_index_hash_table = {str(v):i for i,v in enumerate(index_rep_map)}\n",
    "# Create new image representation that takes color image and replaces all RGB colors with indices\n",
    "count = 0\n",
    "# iterate over colors\n",
    "index_img = np.zeros((32,32))\n",
    "\n",
    "color_img_int = color_img.astype(int)\n",
    "for i in range(color_img_int.shape[0]):\n",
    "    for j in range(color_img_int.shape[1]):\n",
    "        color_string = str(color_img_int[i,j,:])\n",
    "        index_img[i,j] = color_index_hash_table[color_string]\n",
    "index_img = index_img.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 graph init\n",
    "\n",
    "# Create a N X N matrix of zeros\n",
    "n = len(index_rep_map)\n",
    "graph = np.zeros((n,n)).astype(int)\n",
    "\n",
    "# Step 2\n",
    "\n",
    "# horizontal and vertical scans\n",
    "m = index_img.shape[0]\n",
    "horizontal_mark = None\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        # horizontal scan\n",
    "        # make sure j-1 > -1\n",
    "        if j>0:\n",
    "            current_color = index_img[i][j]\n",
    "            previous_color = index_img[i][j-1]\n",
    "            if current_color != previous_color:\n",
    "                graph[current_color][previous_color]+=1\n",
    "                graph[previous_color][current_color]+=1\n",
    "        # vertical scan\n",
    "        # make sure i-1<-1\n",
    "        if i>0:\n",
    "            current_color = index_img[i][j]\n",
    "            previous_color = index_img[i-1][j]\n",
    "            if current_color != previous_color:\n",
    "                graph[current_color][previous_color]+=1\n",
    "                graph[previous_color][current_color]+=1\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399, 399)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_graph = graph.copy()\n",
    "binary_graph[graph>0]=1\n",
    "binary_graph_of_reps_only = binary_graph[1:,1:]\n",
    "binary_graph_of_reps_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "shortest path algorithm - BFS\n",
    "\n",
    "graph representation: adjacency dict\n",
    "\"\"\"\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def adjacency_matrix_to_dict(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    g_dict = {k:set([]) for k in range(n)}\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if matrix[i][j]:\n",
    "                g_dict[i].add(j)\n",
    "    g_dict = {k:list(v) for k,v in g_dict.items()}\n",
    "    return g_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_list = adjacency_matrix_to_dict(binary_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 397, 12]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def BFS(G,start,end):\n",
    "    #assert start in G and end in G, 'start (or) end node not in Graph'\n",
    "    #assert G[start] and G[end], 'start (or) end nodes are disjoint'\n",
    "    q = deque([])\n",
    "    q.append([start])\n",
    "    \n",
    "    visited =set([start])\n",
    "    path = []\n",
    "    \n",
    "    while q:\n",
    "        \n",
    "        path = q.popleft()\n",
    "        node = path[-1]\n",
    "        if node == end:\n",
    "            return path\n",
    "        else:\n",
    "            for adjacent in G.get(node,[node]):\n",
    "                new_path = list(path)\n",
    "                new_path.append(adjacent)\n",
    "                q.append(new_path)\n",
    " \n",
    "    return []\n",
    "\n",
    "BFS(adjacency_list,11,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [257,\n",
       "  2,\n",
       "  255,\n",
       "  387,\n",
       "  383,\n",
       "  137,\n",
       "  395,\n",
       "  12,\n",
       "  143,\n",
       "  17,\n",
       "  273,\n",
       "  276,\n",
       "  23,\n",
       "  28,\n",
       "  158,\n",
       "  33,\n",
       "  34,\n",
       "  163,\n",
       "  164,\n",
       "  169,\n",
       "  42,\n",
       "  298,\n",
       "  46,\n",
       "  47,\n",
       "  63,\n",
       "  320,\n",
       "  323,\n",
       "  68,\n",
       "  72,\n",
       "  73,\n",
       "  329,\n",
       "  341,\n",
       "  86,\n",
       "  351,\n",
       "  98,\n",
       "  354,\n",
       "  357,\n",
       "  104,\n",
       "  106,\n",
       "  362,\n",
       "  367,\n",
       "  127],\n",
       " 1: [384, 2, 4, 392, 399, 16, 112, 88, 94],\n",
       " 2: [0, 1, 384, 3, 4, 7, 167, 73, 169, 170, 394, 397, 17, 52, 377, 58],\n",
       " 3: [33, 2, 9, 169, 367, 55, 24, 58, 159],\n",
       " 4: [1, 2, 392, 16, 52],\n",
       " 5: [16, 9, 52, 396],\n",
       " 6: [],\n",
       " 7: [2, 165, 394, 44, 13, 52, 58],\n",
       " 8: [389, 398, 112, 50, 374, 88],\n",
       " 9: [3, 5, 396, 16, 55, 24, 89],\n",
       " 10: [],\n",
       " 11: [165, 394, 141, 397, 20],\n",
       " 12: [0, 136, 393, 397, 86, 27],\n",
       " 13: [58, 44, 165, 7],\n",
       " 14: [],\n",
       " 15: [],\n",
       " 16: [1, 4, 5, 392, 9, 398, 52, 88, 89],\n",
       " 17: [0, 384, 2, 387, 94],\n",
       " 18: [],\n",
       " 19: [],\n",
       " 20: [168, 393, 11, 109, 141, 397, 378, 27, 93],\n",
       " 21: [],\n",
       " 22: [393, 163],\n",
       " 23: [0, 34, 323, 50, 374, 255],\n",
       " 24: [320, 33, 386, 3, 388, 9, 371, 89, 159],\n",
       " 25: [],\n",
       " 26: [],\n",
       " 27: [393, 12, 20, 397],\n",
       " 28: [0, 387, 42, 46, 47, 63],\n",
       " 29: [320, 34, 371],\n",
       " 30: [],\n",
       " 31: [],\n",
       " 32: [],\n",
       " 33: [0, 320, 3, 367, 24, 159],\n",
       " 34: [0, 320, 323, 50, 23, 29],\n",
       " 35: [],\n",
       " 36: [],\n",
       " 37: [],\n",
       " 38: [],\n",
       " 39: [],\n",
       " 40: [],\n",
       " 41: [],\n",
       " 42: [0, 387, 28, 47],\n",
       " 43: [],\n",
       " 44: [165, 71, 7, 13, 377],\n",
       " 45: [],\n",
       " 46: [0, 143, 28, 63, 127],\n",
       " 47: [0, 42, 28],\n",
       " 48: [],\n",
       " 49: [],\n",
       " 50: [34, 389, 391, 8, 398, 371, 374, 23],\n",
       " 51: [],\n",
       " 52: [2, 4, 5, 7, 170, 394, 396, 16],\n",
       " 53: [],\n",
       " 54: [],\n",
       " 55: [9, 58, 3, 396],\n",
       " 56: [],\n",
       " 57: [],\n",
       " 58: [2, 3, 7, 396, 13, 55],\n",
       " 59: [],\n",
       " 60: [],\n",
       " 61: [],\n",
       " 62: [],\n",
       " 63: [0, 28, 46, 143],\n",
       " 64: [],\n",
       " 65: [],\n",
       " 66: [],\n",
       " 67: [],\n",
       " 68: [0, 378, 104, 351],\n",
       " 69: [],\n",
       " 70: [],\n",
       " 71: [357, 165, 106, 395, 44, 341, 377, 383],\n",
       " 72: [0, 98, 164, 399, 340],\n",
       " 73: [0, 2, 397],\n",
       " 74: [],\n",
       " 75: [],\n",
       " 76: [],\n",
       " 77: [],\n",
       " 78: [],\n",
       " 79: [],\n",
       " 80: [],\n",
       " 81: [],\n",
       " 82: [],\n",
       " 83: [],\n",
       " 84: [],\n",
       " 85: [],\n",
       " 86: [0, 136, 329, 362, 393, 12, 276],\n",
       " 87: [],\n",
       " 88: [1, 8, 398, 112, 16],\n",
       " 89: [16, 9, 398, 24],\n",
       " 90: [],\n",
       " 91: [],\n",
       " 92: [],\n",
       " 93: [163, 109, 20, 378, 158, 351],\n",
       " 94: [384, 1, 387, 399, 17],\n",
       " 95: [],\n",
       " 96: [],\n",
       " 97: [],\n",
       " 98: [0, 72, 366, 255],\n",
       " 99: [],\n",
       " 100: [],\n",
       " 101: [],\n",
       " 102: [],\n",
       " 103: [],\n",
       " 104: [0, 257, 68, 168],\n",
       " 105: [],\n",
       " 106: [0, 357, 341, 71],\n",
       " 107: [],\n",
       " 108: [],\n",
       " 109: [393, 163, 20, 93],\n",
       " 110: [],\n",
       " 111: [],\n",
       " 112: [1, 8, 366, 399, 340, 88],\n",
       " 113: [],\n",
       " 114: [],\n",
       " 115: [],\n",
       " 116: [],\n",
       " 117: [],\n",
       " 118: [],\n",
       " 119: [],\n",
       " 120: [],\n",
       " 121: [],\n",
       " 122: [],\n",
       " 123: [],\n",
       " 124: [],\n",
       " 125: [],\n",
       " 126: [],\n",
       " 127: [0, 137, 46, 329],\n",
       " 128: [],\n",
       " 129: [],\n",
       " 130: [],\n",
       " 131: [],\n",
       " 132: [],\n",
       " 133: [],\n",
       " 134: [],\n",
       " 135: [],\n",
       " 136: [12, 276, 86],\n",
       " 137: [0, 273, 127],\n",
       " 138: [],\n",
       " 139: [],\n",
       " 140: [],\n",
       " 141: [168, 11, 20, 395],\n",
       " 142: [],\n",
       " 143: [0, 46, 63],\n",
       " 144: [],\n",
       " 145: [],\n",
       " 146: [],\n",
       " 147: [],\n",
       " 148: [],\n",
       " 149: [],\n",
       " 150: [],\n",
       " 151: [],\n",
       " 152: [],\n",
       " 153: [],\n",
       " 154: [],\n",
       " 155: [],\n",
       " 156: [],\n",
       " 157: [],\n",
       " 158: [0, 93, 351],\n",
       " 159: [24, 33, 3],\n",
       " 160: [],\n",
       " 161: [],\n",
       " 162: [],\n",
       " 163: [0, 362, 109, 22, 93],\n",
       " 164: [0, 72, 387, 399],\n",
       " 165: [71, 7, 394, 11, 44, 13, 395],\n",
       " 166: [],\n",
       " 167: [169, 2, 377, 383],\n",
       " 168: [257, 104, 395, 141, 20, 378],\n",
       " 169: [0, 2, 3, 354, 167, 367, 383],\n",
       " 170: [2, 52, 394],\n",
       " 171: [],\n",
       " 172: [],\n",
       " 173: [],\n",
       " 174: [],\n",
       " 175: [],\n",
       " 176: [],\n",
       " 177: [],\n",
       " 178: [],\n",
       " 179: [],\n",
       " 180: [],\n",
       " 181: [],\n",
       " 182: [],\n",
       " 183: [],\n",
       " 184: [],\n",
       " 185: [],\n",
       " 186: [],\n",
       " 187: [],\n",
       " 188: [],\n",
       " 189: [],\n",
       " 190: [],\n",
       " 191: [],\n",
       " 192: [],\n",
       " 193: [],\n",
       " 194: [],\n",
       " 195: [],\n",
       " 196: [],\n",
       " 197: [],\n",
       " 198: [],\n",
       " 199: [],\n",
       " 200: [],\n",
       " 201: [],\n",
       " 202: [],\n",
       " 203: [],\n",
       " 204: [],\n",
       " 205: [],\n",
       " 206: [],\n",
       " 207: [],\n",
       " 208: [],\n",
       " 209: [],\n",
       " 210: [],\n",
       " 211: [],\n",
       " 212: [],\n",
       " 213: [],\n",
       " 214: [],\n",
       " 215: [],\n",
       " 216: [],\n",
       " 217: [],\n",
       " 218: [],\n",
       " 219: [],\n",
       " 220: [],\n",
       " 221: [],\n",
       " 222: [],\n",
       " 223: [],\n",
       " 224: [],\n",
       " 225: [],\n",
       " 226: [],\n",
       " 227: [],\n",
       " 228: [],\n",
       " 229: [],\n",
       " 230: [],\n",
       " 231: [],\n",
       " 232: [],\n",
       " 233: [],\n",
       " 234: [],\n",
       " 235: [],\n",
       " 236: [],\n",
       " 237: [],\n",
       " 238: [],\n",
       " 239: [],\n",
       " 240: [],\n",
       " 241: [],\n",
       " 242: [],\n",
       " 243: [],\n",
       " 244: [],\n",
       " 245: [],\n",
       " 246: [],\n",
       " 247: [],\n",
       " 248: [],\n",
       " 249: [],\n",
       " 250: [],\n",
       " 251: [],\n",
       " 252: [],\n",
       " 253: [],\n",
       " 254: [],\n",
       " 255: [0, 98, 374, 23],\n",
       " 256: [],\n",
       " 257: [0, 298, 104, 168],\n",
       " 258: [],\n",
       " 259: [],\n",
       " 260: [],\n",
       " 261: [],\n",
       " 262: [],\n",
       " 263: [],\n",
       " 264: [],\n",
       " 265: [],\n",
       " 266: [],\n",
       " 267: [],\n",
       " 268: [],\n",
       " 269: [],\n",
       " 270: [],\n",
       " 271: [],\n",
       " 272: [],\n",
       " 273: [0, 137, 329],\n",
       " 274: [],\n",
       " 275: [],\n",
       " 276: [0, 329, 136, 86],\n",
       " 277: [],\n",
       " 278: [],\n",
       " 279: [],\n",
       " 280: [],\n",
       " 281: [],\n",
       " 282: [],\n",
       " 283: [],\n",
       " 284: [],\n",
       " 285: [],\n",
       " 286: [],\n",
       " 287: [],\n",
       " 288: [],\n",
       " 289: [],\n",
       " 290: [],\n",
       " 291: [],\n",
       " 292: [],\n",
       " 293: [],\n",
       " 294: [],\n",
       " 295: [],\n",
       " 296: [],\n",
       " 297: [],\n",
       " 298: [0, 257, 395],\n",
       " 299: [],\n",
       " 300: [],\n",
       " 301: [],\n",
       " 302: [],\n",
       " 303: [],\n",
       " 304: [],\n",
       " 305: [],\n",
       " 306: [],\n",
       " 307: [],\n",
       " 308: [],\n",
       " 309: [],\n",
       " 310: [],\n",
       " 311: [],\n",
       " 312: [],\n",
       " 313: [],\n",
       " 314: [],\n",
       " 315: [],\n",
       " 316: [],\n",
       " 317: [],\n",
       " 318: [],\n",
       " 319: [],\n",
       " 320: [0, 33, 34, 371, 24, 29],\n",
       " 321: [],\n",
       " 322: [],\n",
       " 323: [0, 34, 23],\n",
       " 324: [],\n",
       " 325: [],\n",
       " 326: [],\n",
       " 327: [],\n",
       " 328: [],\n",
       " 329: [0, 273, 276, 86, 127],\n",
       " 330: [],\n",
       " 331: [],\n",
       " 332: [],\n",
       " 333: [],\n",
       " 334: [],\n",
       " 335: [],\n",
       " 336: [],\n",
       " 337: [],\n",
       " 338: [],\n",
       " 339: [],\n",
       " 340: [72, 112, 366, 399],\n",
       " 341: [0, 106, 395, 71],\n",
       " 342: [],\n",
       " 343: [],\n",
       " 344: [],\n",
       " 345: [],\n",
       " 346: [],\n",
       " 347: [],\n",
       " 348: [],\n",
       " 349: [],\n",
       " 350: [],\n",
       " 351: [0, 68, 93, 158],\n",
       " 352: [],\n",
       " 353: [],\n",
       " 354: [0, 169],\n",
       " 355: [],\n",
       " 356: [],\n",
       " 357: [0, 106, 383, 71],\n",
       " 358: [],\n",
       " 359: [],\n",
       " 360: [],\n",
       " 361: [],\n",
       " 362: [0, 393, 163, 86],\n",
       " 363: [],\n",
       " 364: [],\n",
       " 365: [],\n",
       " 366: [112, 98, 340, 374],\n",
       " 367: [0, 33, 3, 169],\n",
       " 368: [],\n",
       " 369: [],\n",
       " 370: [],\n",
       " 371: [320, 386, 391, 50, 24, 29],\n",
       " 372: [],\n",
       " 373: [],\n",
       " 374: [389, 8, 366, 50, 23, 255],\n",
       " 375: [],\n",
       " 376: [],\n",
       " 377: [167, 2, 44, 71],\n",
       " 378: [168, 20, 93, 68],\n",
       " 379: [],\n",
       " 380: [],\n",
       " 381: [],\n",
       " 382: [],\n",
       " 383: [0, 357, 167, 71, 169],\n",
       " 384: [1, 2, 17, 94],\n",
       " 385: [],\n",
       " 386: [24, 371, 388],\n",
       " 387: [0, 164, 42, 399, 17, 28, 94],\n",
       " 388: [24, 386, 398, 391],\n",
       " 389: [8, 50, 374],\n",
       " 390: [],\n",
       " 391: [50, 371, 388, 398],\n",
       " 392: [16, 1, 4],\n",
       " 393: [362, 12, 109, 20, 86, 22, 27],\n",
       " 394: [2, 165, 7, 170, 11, 397, 52],\n",
       " 395: [0, 165, 71, 168, 298, 141, 341],\n",
       " 396: [5, 9, 52, 55, 58],\n",
       " 397: [2, 73, 394, 11, 12, 20, 27],\n",
       " 398: [388, 391, 8, 16, 50, 88, 89],\n",
       " 399: [1, 387, 164, 72, 112, 340, 94]}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ASIDE: what's a better way to construct a graph?\n",
    "\n",
    "Currently, using the color image as ground truth is a bad a idea because it is a lossy representation of actual\n",
    "connections.\n",
    "\n",
    "The best way to build a graph is to run VQ VAE on data and keep track of transitions \n",
    "\n",
    "e.g. each time z_i -> z_j and i != j, hash the representation and +=1 in some monster adjacency dictionary\n",
    "to keep the representations fixed, you can pick to N reps and then only count if the transition is in one of those \n",
    "reps\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run local policy with a reward specified from global policy\n",
    "\n",
    "Given start observation o_1 and end observation o_2, construct plan with graph\n",
    "\n",
    "E.g.\n",
    "suppose o_1 -> z_7 and o_2 -> z_9 construct graph z_7 -> z_0 -> z_1 -> z_9\n",
    "and then train goal conditioned policy that needs to solve curriculum\n",
    "pi(a|o,z_goal) where z_goal = z_0 then z_1 and then z_9 depending on which current z_j you're in\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "What kind of policy should we run?\n",
    "\n",
    "Off-policy:\n",
    "\n",
    "In the off-policy case, we can collect a bunch of data (i.e. the space will be explored), we can then \n",
    "- collect a replay buffer of observations\n",
    "- then train a VQ VAE\n",
    "- then create a replay buffer with latents \n",
    "- then learn a goal conditioned policy\n",
    "\n",
    "How do we learn goal conditioned policies? Try the HER thing\n",
    "\n",
    "HER thing: input concatenated state || goal and train policy that way\n",
    "\n",
    "how does it work with representation learning?\n",
    "\n",
    "- after each trajectory (z_1,_z_1,z_17,z_5,z_5, etc) relabel as though agent was trying to achieve\n",
    "some latent state in the trajectory (e.g. z_j) and store transitions (z_i || z_j) in the replay buffer\n",
    "\n",
    "(Notes:\n",
    "- by the way you can go through ALL the representations achieved\n",
    "- maybe best to just reward the final latent observation so that it is always forward thinking\n",
    ")\n",
    "- after this policy is trained it should be able to arrive at any latent goal state z_i\n",
    "\n",
    "how do we treat representations - i.e. use entire matrix or an encoding?\n",
    "\n",
    "\n",
    "PLAN:\n",
    "\n",
    "Try with Block world. \n",
    "- Train VQ VAE on random block world\n",
    "- Train HER on latent representation \n",
    "    - flatten the 8x8 latent image into a 64 dim vector\n",
    "- Optional: provide .1 points for moving to different representation than current one, and 1 point for moving to\n",
    "  HER representation \n",
    "- use DDPG or TD3 variant (continuous actions)\n",
    "- At test time make a plan z_1 - z_2 - z_N and have agent reach each goal individually\n",
    "- Retrain VQ VAE periodically, and reconstruct graph\n",
    "\n",
    "options for reward relabling\n",
    "\n",
    "option 1: teach agent to reach any goal\n",
    "1. train time - normal HER\n",
    "2. test time - make a plan z_1 -> z_2 ... -> z_g and have policy follow plan\n",
    "\n",
    "option 2: teach agent to reach goal sequence\n",
    "1. train time - pick random future goal like in HER, find graph to said goal in path and reward path states\n",
    "2. test time - same as (2) previously\n",
    "\n",
    "option 1 is simpler let's do that\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FUTURE:\n",
    "\n",
    "train in robotic simulators e.g. fetch reach\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "how do we make a better / cleaner graph?\n",
    "\n",
    "first, run a VQ VAE over the data and construct the graph from transitions NOT from the final color image\n",
    "\n",
    "we can choose N representations put them in set S and run the VQ VAE over the data\n",
    "then if the encoding is inside the set S, we keep it, if not we set it to the null encoding \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
